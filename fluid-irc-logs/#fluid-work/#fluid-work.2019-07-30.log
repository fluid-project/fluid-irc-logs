2019-07-30T01:25:13  *** Justin_o <Justin_o!uid14648@gateway/web/irccloud.com/x-jnvyulptmnrdqyri> has quit IRC (Quit: Connection closed for inactivity)
2019-07-30T01:57:43  *** colinclark <colinclark!~colinclar@192-0-158-138.cpe.teksavvy.com> has joined #fluid-work
2019-07-30T04:19:29  *** colinclark <colinclark!~colinclar@192-0-158-138.cpe.teksavvy.com> has quit IRC (Quit: colinclark)
2019-07-30T06:45:36  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has joined #fluid-work
2019-07-30T08:33:03  *** kris_HA <kris_HA!~yanachkov@78.128.42.5> has joined #fluid-work
2019-07-30T08:34:37  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has quit IRC (Ping timeout: 245 seconds)
2019-07-30T08:43:38  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has joined #fluid-work
2019-07-30T08:45:53  *** kris_HA <kris_HA!~yanachkov@78.128.42.5> has quit IRC (Ping timeout: 245 seconds)
2019-07-30T09:20:10  *** Bosmon <Bosmon!~a@82-71-9-15.dsl.in-addr.zen.co.uk> has quit IRC (Ping timeout: 246 seconds)
2019-07-30T09:51:31  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3e5a:900:d9e0:6594:11c4:1e82> has joined #fluid-work
2019-07-30T09:52:33  *** ussjoin_ <ussjoin_!~ussjoin@brighid.ussjoin.com> has quit IRC (Ping timeout: 245 seconds)
2019-07-30T09:57:39  *** ussjoin <ussjoin!~ussjoin@brighid.ussjoin.com> has joined #fluid-work
2019-07-30T10:23:16  *** Danail_Dido <Danail_Dido!~Karadalie@109.120.246.31> has joined #fluid-work
2019-07-30T10:23:30  *** Danail_Dido <Danail_Dido!~Karadalie@109.120.246.31> has quit IRC (Client Quit)
2019-07-30T11:43:47  *** kris_HA <kris_HA!~yanachkov@78.128.42.5> has joined #fluid-work
2019-07-30T11:45:46  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has quit IRC (Ping timeout: 268 seconds)
2019-07-30T11:53:03  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has joined #fluid-work
2019-07-30T11:54:19  *** Bosmon <Bosmon!~a@82-71-9-15.dsl.in-addr.zen.co.uk> has joined #fluid-work
2019-07-30T11:55:34  *** kris_HA <kris_HA!~yanachkov@78.128.42.5> has quit IRC (Ping timeout: 246 seconds)
2019-07-30T12:09:36  *** cherylhjli <cherylhjli!~Adium@lnsm2-toronto47-142-116-104-176.internet.virginmobile.ca> has joined #fluid-work
2019-07-30T12:09:50  *** avtar <avtar!~avtar@ip-24-50-176-117.user.start.ca> has joined #fluid-work
2019-07-30T12:29:21  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has joined #fluid-work
2019-07-30T12:30:48  *** cherylhjli <cherylhjli!~Adium@lnsm2-toronto47-142-116-104-176.internet.virginmobile.ca> has quit IRC (Quit: Leaving.)
2019-07-30T12:32:23  *** simonjb <simonjb!~simonjb@198.178.118.18> has joined #fluid-work
2019-07-30T12:37:56  *** cherylhjli <cherylhjli!~Adium@lnsm2-toronto47-142-116-104-176.internet.virginmobile.ca> has joined #fluid-work
2019-07-30T13:02:28  *** cindyli <cindyli!Adium@nat/ocadu/x-lhqvxizzoiahabgh> has joined #fluid-work
2019-07-30T13:18:01  *** cindyli <cindyli!Adium@nat/ocadu/x-lhqvxizzoiahabgh> has quit IRC (Quit: Leaving.)
2019-07-30T13:19:12  *** cindyli <cindyli!Adium@nat/ocadu/x-umfnvlvuoailxfgf> has joined #fluid-work
2019-07-30T13:23:57  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has quit IRC (Remote host closed the connection)
2019-07-30T13:24:17  *** Justin_o <Justin_o!uid14648@gateway/web/irccloud.com/x-orvkbswpfpgbmrud> has joined #fluid-work
2019-07-30T13:24:19  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has joined #fluid-work
2019-07-30T13:26:08  *** cherylhjli <cherylhjli!~Adium@lnsm2-toronto47-142-116-104-176.internet.virginmobile.ca> has quit IRC (Quit: Leaving.)
2019-07-30T13:29:21  *** kendraf <kendraf!Adium@nat/ocadu/x-ncshuyvbqmuwhcxw> has joined #fluid-work
2019-07-30T13:33:56  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.105> has joined #fluid-work
2019-07-30T13:35:39  *** clown <clown!clown@nat/ocadu/x-uidgwjzagpavxdqh> has joined #fluid-work
2019-07-30T13:38:34  *** alanharnum <alanharnum!uid363993@gateway/web/irccloud.com/x-dhyqqdclqupjjnvq> has joined #fluid-work
2019-07-30T13:41:56  <Justin_o> alanharnum: do you know if we have access to IRC logs that pre-date 2018-09-21?
2019-07-30T13:43:02  *** jhung is now known as jhung_away
2019-07-30T13:43:33  <alanharnum> Justin_o: Yes, everything from BotBot is available here: https://github.com/waharnum/fluid-irc-logs ( we should transfer this to fluid-project)
2019-07-30T13:43:48  *** clown <clown!clown@nat/ocadu/x-uidgwjzagpavxdqh> has quit IRC (Ping timeout: 245 seconds)
2019-07-30T13:44:11  <Justin_o> alanharnum: yes, that would be helpful. can we also add a link in the current logs directory that points to these?
2019-07-30T13:45:20  <alanharnum> Justin_o: It's currently linked from https://wiki.fluidproject.org/display/fluid/IRC+Channel - agree a link from the logs directory would be helpful but that would take a bit of work. Right now we just mount the logging bot's directories to a web server.
2019-07-30T13:46:08  <Justin_o> okay
2019-07-30T13:46:18  <Justin_o> maybe something we can do when we have some time
2019-07-30T13:47:23  <alanharnum> Justin_o: I've opened a transfer of the repo to you - if you take ownership you should then be able to transfer it to fluid-project (I don't have repo create access there).
2019-07-30T13:47:40  <Justin_o> okay.. I'll look into that now
2019-07-30T13:55:57  *** clown <clown!clown@nat/ocadu/x-xphicmkycxnzyzsn> has joined #fluid-work
2019-07-30T14:00:19  <Bosmon> Justin_o - thanks for locating the ancient conversation :)
2019-07-30T14:00:59  <Justin_o> no problem, thankfully we have logs.. I only had a vague recollection of the conversation and couldn't remember the reasoning
2019-07-30T14:01:19  *** crystalchan <crystalchan!Adium@nat/ocadu/x-pdxmsrajgnoqaacd> has joined #fluid-work
2019-07-30T14:03:49  <Justin_o> alanharnum: fluid-irc-logs repo has been transferred to fluid-project
2019-07-30T14:03:50  <Justin_o> https://github.com/fluid-project/fluid-irc-logs
2019-07-30T14:04:32  <Bosmon> Justin_o - well, I have to say, this advice you received doesn't sound great : P
2019-07-30T14:04:58  <Bosmon> But I guess what with the repeated CI failures going on that day I didn't feel up to getting a new promise utility incorporated
2019-07-30T14:05:23  <Bosmon> Whilst you were away simonjb also queried the semantics and suitability of fluid.promise.sequence which is currently our only real "workhorse"
2019-07-30T14:06:03  <Bosmon> Let me at least write up a JIRA so you have something to refer to in your code to deter casual critics
2019-07-30T14:07:33  *** jhung_away <jhung_away!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has quit IRC (Ping timeout: 245 seconds)
2019-07-30T14:07:45  <simonjb> Bosmon: I created a JIRA to add to the fluid.sequence.promise docs https://issues.fluidproject.org/browse/FLUID-6374
2019-07-30T14:07:48  <Justin_o> Bosmon: thanks :)
2019-07-30T14:08:07  <simonjb> and I did write the docs, but didn't make a PR (sorry!)
2019-07-30T14:08:16  <simonjb> let me find the docs
2019-07-30T14:08:21  <Bosmon> Justin_o - ah well, it actually looks like I already wrote up this JIRA 3 years ago : P
2019-07-30T14:08:22  <Bosmon> https://issues.fluidproject.org/browse/FLUID-5938
2019-07-30T14:08:43  <Bosmon> I'll add a further PR comment
2019-07-30T14:09:38  <simonjb> https://github.com/fluid-project/infusion-docs/compare/master...simonbates:FLUID-6374
2019-07-30T14:09:53  <simonjb> I'll make the PR today
2019-07-30T14:10:10  <Justin_o> Bosmon: would this new utility help my case though, as I'd still want the "sequence" to resolve even though something was rejected.. although I guess I could fire afterSyllabifcation in either the resolved or rejected case
2019-07-30T14:10:40  <Bosmon> Justin_o - well, that's what this utility does
2019-07-30T14:10:54  <Bosmon> It waits till each promise in the structure is "settled", either through resolve or reject
2019-07-30T14:11:26  <Bosmon> And also it is inherently more suitable since it doesn't talk about "sequencing" which as the earlier conversation that November day established, you don't really want anyway
2019-07-30T14:12:04  <Bosmon> It's actually massively suitable since you wouldn't even need to do any further work other than simply passing the whole of that.hyphenators into fluid.promise.settleStructure as it stands
2019-07-30T14:12:11  <Justin_o> Bosmon: it's this part of the sentence that's confusing me I guess "or a rejection if any promise rejects"
2019-07-30T14:12:34  <Justin_o> so I'd be able to just drop this in?
2019-07-30T14:12:42  <Justin_o> is it something I can just add in place now?
2019-07-30T14:12:57  <Justin_o> that is, can I just migrate the code as part of this PR?
2019-07-30T14:13:25  <Bosmon> Justin_o - the sentence is written wrongly, let me fix it
2019-07-30T14:15:09  <Justin_o> thanks
2019-07-30T14:15:12  <Bosmon> Justin_o - ok, it's not that the sentence is written wrongly, the impl is written badly :)
2019-07-30T14:15:27  <Justin_o> haha
2019-07-30T14:17:36  *** jhung_away <jhung_away!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has joined #fluid-work
2019-07-30T14:17:48  *** jhung_away is now known as jhung
2019-07-30T14:21:52  <Bosmon> Justin_o - ok, I've added a comment to FLUID-5938 explaining how the impl should be improved
2019-07-30T14:22:12  <Bosmon> And of course, once we did incorporate this into the framework, it would need some reasonable set of tests : P
2019-07-30T14:22:56  <Bosmon> Justin_o - but it's clear that this is a widely relevant algorithm that we need since I've since ended up using it in some of my own work at https://github.com/amb26/bagatelle/blob/master/src/utils/settleStructure.js
2019-07-30T14:26:45  <javjarfer[m]> Bosmon: thanks for the merge!
2019-07-30T14:27:39  <javjarfer[m]> cindyli: hi, good afternoon, do you know if we have any special setting in the flowmanager runtime limiting the amount of requests it should handle? or connections?
2019-07-30T14:28:30  <Bosmon> javjarfer[m] - there is no such setting
2019-07-30T14:28:32  <Justin_o> Bosmon: thanks for updating the JIRA, and for the example.
2019-07-30T14:31:28  <javjarfer[m]> Bosmon: I see, I have been doing some performance experiments past week against my cloud env, and it wasn't being able to resolve many concurrent requests
2019-07-30T14:32:31  <Bosmon> javjarfer[m] - what would it do?
2019-07-30T14:32:33  <javjarfer[m]> preliminary results are interesting because even if many requests are responded with 503 errors, the CPU usage of the cluster is low, and I have only been able to increase throughput by increasing the number of flowmanagers
2019-07-30T14:33:10  <Bosmon> javjarfer[m] - what do the text of the 503 errors say?
2019-07-30T14:33:26  <Bosmon> And what was the level of concurrency when you received them?
2019-07-30T14:35:13  <javjarfer[m]> Bosmon:  Depending on the the level of concurrency of the requests it behaves differently, if the concurrency is below 20 (for a small cluster of 2 flowmanagers) for 300 requests starts answering with 200 and you may found some 503, with a maximum throughput of 6 - 8 req / s
2019-07-30T14:35:46  <Bosmon> javjarfer[m] - the only kind of limitation I can find by a quick search is that the default limit at the POSIX level is 1024 open file descriptors
2019-07-30T14:35:48  <javjarfer[m]> If you increase the number of concurrency, up 35-40 and above, you start getting  much more 503
2019-07-30T14:36:32  <javjarfer[m]> and if you reach 60 level of concurrency then most of the requests are dropped and only a few of them are answer 0-5 range
2019-07-30T14:37:27  <Bosmon> javjarfer[m] - that's pretty interesting - if the mechanism involved was based on the file descriptors it would be consistent with each request issuing a further 30 or so I/O requests by itself
2019-07-30T14:37:34  <Bosmon> Which sounds like far more than is plausible
2019-07-30T14:37:40  <Bosmon> But I can't think of another mechanism at present
2019-07-30T14:37:42  * javjarfer[m] sent a long message:  < https://matrix.org/_matrix/media/v1/download/matrix.org/dFcyphgQGhBrPqHBMHWUhVcG >
2019-07-30T14:38:01  <Bosmon> javjarfer[m] - there's certainly nothing we are doing at the application level to generate a 503 code
2019-07-30T14:39:51  <javjarfer[m]> Bosmon: so, each request involves 30 opened files descriptors?
2019-07-30T14:46:58  <jhung> fluid-everyone: IDRC folks - do we want to do planning today? I'm okay with skipping this week as long as we update the iteration plan page.
2019-07-30T14:47:14  <javjarfer[m]> If this is the case, I can increase the max number of file descriptors for the flowmanager instances by hand in the cluster and retry
2019-07-30T14:51:53  <Bosmon> javjarfer[m] - no, I was just trying to make a rough estimate of how many file descriptors would have to be opened in order to account for this failure through the mechanism of a 1024 FD limit
2019-07-30T14:52:10  <Justin_o> jhung: I'm fine either way
2019-07-30T14:52:37  <javjarfer[m]> Bosmon: okay, I see, I will try increasing it and testing again
2019-07-30T14:54:42  <jhung> clown, alanharnum, cindyli, gmoss, sepidehshahi, simonjb, avtar, greatislander - are you okay with skipping planning today?
2019-07-30T14:54:53  <Bosmon> javjarfer[m] - well, it might be worth just querying what the FD count is for the process while you are running the test
2019-07-30T14:54:59  <alanharnum> jhung - this is fine with me
2019-07-30T14:55:04  <cindyli> fine with me too, jhernandez
2019-07-30T14:55:09  <clown> jhung, skipping is okay with me
2019-07-30T14:55:10  <greatislander> okay with me jhung
2019-07-30T14:55:10  <simonjb> jhung: fine with me also
2019-07-30T14:55:27  <cindyli> it was for you, jhung
2019-07-30T14:55:29  <avtar> jhung: yes
2019-07-30T14:55:38  <jhung> ok. please try to update the iteration plan page. :) Thanks!
2019-07-30T14:55:47  <Bosmon> javjarfer[m] - the text of that failure suggests that the message was generated by istio
2019-07-30T14:56:01  <Bosmon> So I recommend that you check with the ops folks to see if they are aware of any configured limitations themselves
2019-07-30T14:56:13  <Bosmon> javjarfer[m] - see the following report - https://github.com/istio/istio/issues/13205
2019-07-30T15:01:01  <javjarfer[m]> Bosmon: I already commented this with some ops members, particularly stepan, which is the one in charge of istio, he told me that the message means that flowmanager has denied connection to istio when trying to deliver the request, but I will forward him the issue to see his thoughts about it
2019-07-30T15:01:19  <Bosmon> javjarfer[m] - ok, very good
2019-07-30T15:01:37  <Bosmon> Let's go back to the FD investigation then
2019-07-30T15:02:51  *** cindyli <cindyli!Adium@nat/ocadu/x-umfnvlvuoailxfgf> has quit IRC (Read error: Connection reset by peer)
2019-07-30T15:03:33  *** cindyli <cindyli!Adium@nat/ocadu/x-yrwwgeajfrpyuprr> has joined #fluid-work
2019-07-30T15:03:45  <Bosmon> javjarfer[m] - of course it would be more helpful if istio gave us the exact failure condition rather than this catchall
2019-07-30T15:04:02  <Bosmon> e.g. was it connect error, a disconnect, or a reset
2019-07-30T15:05:29  <Bosmon> javjarfer[m] - it might be more helpful to perform these tests in isolation in a simple vagrant box with a copy of universal on it, so we can cut istio out of the picture
2019-07-30T15:08:19  <javjarfer[m]> Bosmon: I agree, if the ulimit increase doesn't change anything, I will go with a local container and I will perform the same tests against it
2019-07-30T15:09:53  <Bosmon> javjarfer[m] - previous rounds of performance tests have suggested that our primary bottleneck relates to our use of CouchDB, with one incoming request perhaps causing something like 7 HTTP requests to CouchDB, some of which have service times of about 100ms
2019-07-30T15:10:20  <Bosmon> But your discovery that you can get more concurrency with more FlowManagers is interesting
2019-07-30T15:15:32  <javjarfer[m]> Bosmon: yes, that's really weird
2019-07-30T15:29:22  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has quit IRC (Ping timeout: 272 seconds)
2019-07-30T15:32:58  *** cherylhjli <cherylhjli!~Adium@205.211.168.101> has joined #fluid-work
2019-07-30T15:39:14  *** colinclark <colinclark!~colinclar@205.211.168.105> has joined #fluid-work
2019-07-30T16:19:41  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has quit IRC (Ping timeout: 258 seconds)
2019-07-30T16:19:44  *** cindyli <cindyli!Adium@nat/ocadu/x-yrwwgeajfrpyuprr> has quit IRC (Quit: Leaving.)
2019-07-30T16:19:51  *** cindyli <cindyli!Adium@nat/ocadu/x-yltlfaxobxulefof> has joined #fluid-work
2019-07-30T16:25:46  *** greatislander <greatislander!sid364905@gateway/web/irccloud.com/x-cgqkhzacdywoszjt> has quit IRC (Ping timeout: 252 seconds)
2019-07-30T16:28:06  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has joined #fluid-work
2019-07-30T16:28:22  *** greatislander <greatislander!sid364905@gateway/web/irccloud.com/x-gcfixzcsspwfvrjy> has joined #fluid-work
2019-07-30T16:29:01  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3e5a:900:d9e0:6594:11c4:1e82> has quit IRC (Quit: jhernandez)
2019-07-30T16:33:04  *** kendraf <kendraf!Adium@nat/ocadu/x-ncshuyvbqmuwhcxw> has quit IRC (Quit: Leaving.)
2019-07-30T16:54:31  *** cherylhjli <cherylhjli!~Adium@205.211.168.101> has quit IRC (Read error: Connection reset by peer)
2019-07-30T16:54:43  *** cherylhjli <cherylhjli!Adium@nat/ocadu/x-nevcokjjevdxwrvt> has joined #fluid-work
2019-07-30T16:56:23  *** cindyli1 <cindyli1!~Adium@205.211.168.102> has joined #fluid-work
2019-07-30T16:58:48  *** cindyli <cindyli!Adium@nat/ocadu/x-yltlfaxobxulefof> has quit IRC (Ping timeout: 245 seconds)
2019-07-30T16:58:56  *** cherylhjli <cherylhjli!Adium@nat/ocadu/x-nevcokjjevdxwrvt> has quit IRC (Client Quit)
2019-07-30T17:13:51  *** cherylhjli <cherylhjli!Adium@nat/ocadu/x-dvpmpitozviafern> has joined #fluid-work
2019-07-30T17:19:28  *** cindyli1 <cindyli1!~Adium@205.211.168.102> has quit IRC (Quit: Leaving.)
2019-07-30T17:35:00  *** kendraf <kendraf!Adium@nat/ocadu/x-anyoxvrnylddqsgj> has joined #fluid-work
2019-07-30T17:48:57  *** avtar <avtar!~avtar@ip-24-50-176-117.user.start.ca> has quit IRC (Quit: Leaving.)
2019-07-30T17:50:53  <jhung> fluid-everyone: The design crit will be starting in 10 minutes. Today we're talking about UIO features.
2019-07-30T17:51:12  *** clown <clown!clown@nat/ocadu/x-xphicmkycxnzyzsn> has quit IRC (Quit: Leaving.)
2019-07-30T17:52:00  *** clown <clown!~clown@205.211.168.102> has joined #fluid-work
2019-07-30T18:02:20  *** kendraf <kendraf!Adium@nat/ocadu/x-anyoxvrnylddqsgj> has quit IRC (Quit: Leaving.)
2019-07-30T18:02:47  *** crystalchan <crystalchan!Adium@nat/ocadu/x-pdxmsrajgnoqaacd> has quit IRC (Quit: Leaving.)
2019-07-30T18:03:58  *** cindyli <cindyli!~Adium@205.211.168.102> has joined #fluid-work
2019-07-30T18:10:56  <clown> cindyli:  filling in the auth: option did the trick for the "add users" test  :-D
2019-07-30T18:11:52  <cindyli> yay! thanks for letting me know, clown
2019-07-30T18:13:01  <clown> progress… millimetre by millimetre
2019-07-30T18:34:53  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.105> has quit IRC (Quit: sepidehshahi)
2019-07-30T18:48:45  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.105> has joined #fluid-work
2019-07-30T18:57:37  *** clown <clown!~clown@205.211.168.102> has quit IRC (Quit: Leaving.)
2019-07-30T19:02:48  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has quit IRC (Ping timeout: 272 seconds)
2019-07-30T19:08:08  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has joined #fluid-work
2019-07-30T19:20:51  *** clown <clown!clown@nat/ocadu/x-vuziskaebliottsl> has joined #fluid-work
2019-07-30T19:46:16  <cindyli> gmoss: check in?
2019-07-30T19:46:49  <gmoss> cindyli: I'm in the ILDH planning meeting right now, we could check in after if the timing works for you
2019-07-30T19:47:45  <cindyli> gmoss: do you know when the meeting ends?
2019-07-30T19:48:08  <gmoss> cindyli: we started a little late so i'll say 4:10 at the latest
2019-07-30T19:48:13  <cindyli> o
2019-07-30T19:48:15  <cindyli> k
2019-07-30T19:48:16  <gmoss> but we could just check in tomorrow
2019-07-30T19:48:38  <gmoss> i haven't got to the programming part of my day yet :) i saw your review thouhg
2019-07-30T19:49:01  <cindyli> ok. let's check in tomorrow.
2019-07-30T20:21:15  *** cindyli <cindyli!~Adium@205.211.168.102> has quit IRC (Quit: Leaving.)
2019-07-30T20:23:33  *** kendraf <kendraf!Adium@nat/ocadu/x-ewvlbxkyhstpgjsl> has joined #fluid-work
2019-07-30T20:23:58  *** crystalchan <crystalchan!Adium@nat/ocadu/x-flihywltfacsokpu> has joined #fluid-work
2019-07-30T20:37:23  *** kendraf <kendraf!Adium@nat/ocadu/x-ewvlbxkyhstpgjsl> has quit IRC (Quit: Leaving.)
2019-07-30T20:38:21  *** crystalchan <crystalchan!Adium@nat/ocadu/x-flihywltfacsokpu> has quit IRC (Quit: Leaving.)
2019-07-30T20:45:19  *** colinclark <colinclark!~colinclar@205.211.168.105> has quit IRC (Quit: colinclark)
2019-07-30T21:00:00  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.105> has quit IRC (Quit: sepidehshahi)
2019-07-30T21:01:11  *** clown <clown!clown@nat/ocadu/x-vuziskaebliottsl> has quit IRC (Quit: Leaving.)
2019-07-30T21:01:19  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has quit IRC (Read error: Connection reset by peer)
2019-07-30T21:09:38  *** avtar <avtar!~avtar@ip-24-50-176-117.user.start.ca> has joined #fluid-work
2019-07-30T21:16:14  *** simonjb <simonjb!~simonjb@198.178.118.18> has quit IRC ()
2019-07-30T21:35:18  *** cherylhjli <cherylhjli!Adium@nat/ocadu/x-dvpmpitozviafern> has quit IRC (Ping timeout: 268 seconds)
2019-07-30T22:31:07  *** colinclark <colinclark!~colinclar@192-0-158-138.cpe.teksavvy.com> has joined #fluid-work
2019-07-30T22:42:35  *** Justin_o <Justin_o!uid14648@gateway/web/irccloud.com/x-orvkbswpfpgbmrud> has quit IRC (Quit: Connection closed for inactivity)
2019-07-30T23:41:46  *** avtar <avtar!~avtar@ip-24-50-176-117.user.start.ca> has quit IRC (Quit: Leaving.)
