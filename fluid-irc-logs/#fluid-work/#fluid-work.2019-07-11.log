2019-07-11T02:33:40  *** avtar <avtar!uid113407@gateway/web/irccloud.com/x-bcmgmmhrsevtstal> has quit IRC (Quit: Connection closed for inactivity)
2019-07-11T06:46:47  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has joined #fluid-work
2019-07-11T09:48:20  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has quit IRC (Ping timeout: 268 seconds)
2019-07-11T09:51:09  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has joined #fluid-work
2019-07-11T10:06:58  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3e5a:900:d9e0:6594:11c4:1e82> has joined #fluid-work
2019-07-11T11:04:35  *** Justin_o <Justin_o!uid14648@gateway/web/irccloud.com/x-oupiwurvyebpmkjh> has joined #fluid-work
2019-07-11T12:27:05  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has joined #fluid-work
2019-07-11T12:34:58  *** michelled <michelled!~Adium@135-23-84-147.cpe.pppoe.ca> has joined #fluid-work
2019-07-11T12:55:10  *** cindyli <cindyli!~Adium@198.52.178.28> has joined #fluid-work
2019-07-11T13:23:18  *** avtar <avtar!~avtar@ip-45-3-26-3.user.start.ca> has joined #fluid-work
2019-07-11T13:24:35  <jhung> Justin_o there's a work-around for the docpad plugin issue that the-t-in-rtf helped resolve while you were away. See the conversation here: https://issues.fluidproject.org/browse/FLUID-6324
2019-07-11T13:25:35  *** kendraf <kendraf!Adium@nat/ocadu/x-qcblnnisnfccdljh> has joined #fluid-work
2019-07-11T13:39:46  <Justin_o> jhung: thanks, i'll take a look at that
2019-07-11T13:42:58  <jhung> but I'll take a look at your PR for the fluid site now.
2019-07-11T13:43:19  <jhung> ^ Justin_o
2019-07-11T13:45:07  *** Bosmon <Bosmon!~a@82-71-9-15.dsl.in-addr.zen.co.uk> has joined #fluid-work
2019-07-11T13:46:06  <Justin_o> jhung: thanks :)
2019-07-11T13:46:09  <Bosmon> Hi jhernandez, cindyli, the-t-in-rtf - my "revert" of the request validation stuff is ready - https://github.com/GPII/universal/pull/802
2019-07-11T13:46:22  *** simonjb <simonjb!~simonjb@198.178.118.18> has joined #fluid-work
2019-07-11T13:46:23  <cindyli> thanks, Bosmon
2019-07-11T13:46:33  <Bosmon> I decided on a "soft revert" in the end since the-t-in-rtf commits contained so many other useful changes, especially, ones that sgithens is relying on
2019-07-11T13:46:43  <Bosmon> So I simply removed the connection between the request grades and the validation machinery
2019-07-11T13:47:13  <Bosmon> This should restore the performance to the baseline and allow other changes to universal to be tested in staging in the meantime
2019-07-11T13:47:17  <Bosmon> jhernandez - merge if you like : P
2019-07-11T13:51:15  *** cherylhjli <cherylhjli!~Adium@205.211.168.101> has joined #fluid-work
2019-07-11T13:55:05  *** clown <clown!clown@nat/ocadu/x-kcoglfkhkyyrphcd> has joined #fluid-work
2019-07-11T13:58:04  <cindyli> Bosmon: we probably should do a performance test with this soft revert?
2019-07-11T13:58:25  <Bosmon> cindyli - I guess so, it's a shame, given that itself will take some time
2019-07-11T14:00:46  <Bosmon> jhernandez - I've updated my "indicative fix" branch with master - https://github.com/GPII/gpii-app/pull/117
2019-07-11T14:01:03  <cindyli> the-t-in-rtf: how do you do the performance test? the only way i know about is the load test that the ops team does after a deployment. wonder if there's an easy way
2019-07-11T14:03:16  <the-t-in-rtf> Given that their smoke tests are the ones blocking deployment of the validation stuff I use those.
2019-07-11T14:03:24  <the-t-in-rtf> Eventually we will need to test a wider range of things
2019-07-11T14:03:36  <the-t-in-rtf> it's just rake test_preferences from your dev cloud setup
2019-07-11T14:04:52  <jhernandez> Bosmon: awesome!!!
2019-07-11T14:04:59  <jhernandez> thanks for these 2 PRs
2019-07-11T14:05:02  <jhernandez> :)
2019-07-11T14:05:42  <the-t-in-rtf> If the "calculate tag" build has come through I'm happy to test the performance in the background, @cindyli
2019-07-11T14:05:52  <jhernandez> and yeah, much better to apply a "soft" revert
2019-07-11T14:06:09  <the-t-in-rtf> doesn't seem like it's come through yet
2019-07-11T14:06:10  <the-t-in-rtf> https://ci.gpii.net/job/docker-gpii-universal-master-calculate-tag/
2019-07-11T14:06:21  <jhernandez> the-t-in-rtf: yeah, it takes some time
2019-07-11T14:06:43  <jhernandez> but it's not been merged yet
2019-07-11T14:06:48  <the-t-in-rtf> ah.
2019-07-11T14:06:56  <jhernandez> so it'll take a bit more ;)
2019-07-11T14:06:57  <the-t-in-rtf> once it's merged I can pull the upstream changes and test locally without waiting
2019-07-11T14:07:11  <the-t-in-rtf> there's a way to do it without waiting, which is what I'm using to test my work.
2019-07-11T14:09:04  <jhernandez> the-t-in-rtf: great, thanks
2019-07-11T14:09:17  <jhernandez> will merge it right away
2019-07-11T14:21:36  <jhernandez> Bosmon, the-t-in-rtf: SHIPPED!
2019-07-11T14:23:24  <javjarfer[m]> hi there cindyli !
2019-07-11T14:23:51  <javjarfer[m]> do you know if there is way to query the deviceReporter right now?
2019-07-11T14:24:36  *** colinclark <colinclark!~colinclar@192-0-158-138.cpe.teksavvy.com> has joined #fluid-work
2019-07-11T14:30:17  *** simonjb <simonjb!~simonjb@198.178.118.18> has quit IRC ()
2019-07-11T14:32:34  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-ijysehoyprybfwcs> has joined #fluid-work
2019-07-11T14:41:23  *** crystalchan <crystalchan!Adium@nat/ocadu/x-cnztdqwskjzynfws> has joined #fluid-work
2019-07-11T14:51:33  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has joined #fluid-work
2019-07-11T14:56:02  *** clown <clown!clown@nat/ocadu/x-kcoglfkhkyyrphcd> has quit IRC (Quit: Leaving.)
2019-07-11T14:57:58  *** clown <clown!clown@nat/ocadu/x-xymhtlbiagihzxnf> has joined #fluid-work
2019-07-11T15:12:13  *** simonjb <simonjb!~simonjb@205.211.168.102> has joined #fluid-work
2019-07-11T15:33:17  *** colinclark <colinclark!~colinclar@192-0-158-138.cpe.teksavvy.com> has quit IRC (Quit: colinclark)
2019-07-11T15:34:11  <cindyli> sorry, javjarfer[m], just saw your message, my adium prompt didn't give an alert. deviceReporter has a get() invoker that returns a promise whose resolved value is solutions installed on that device
2019-07-11T15:34:43  <cindyli> https://github.com/GPII/universal/blob/master/gpii/node_modules/deviceReporter/src/DeviceReporter.js#L62-L65
2019-07-11T15:35:07  <cindyli> also the line 109 for the dynamic device reporter get() API
2019-07-11T15:36:50  <cindyli> an example of how to use this API: https://github.com/GPII/universal/blob/master/gpii/node_modules/lifecycleManager/src/PrivateMatchMaker.js#L67-L74
2019-07-11T15:42:54  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-ijysehoyprybfwcs> has quit IRC (Quit: sepidehshahi)
2019-07-11T15:46:11  *** yanachkov__ <yanachkov__!~yanachkov@185.97.75.80> has quit IRC (Ping timeout: 244 seconds)
2019-07-11T15:48:51  *** avtar <avtar!~avtar@ip-45-3-26-3.user.start.ca> has quit IRC (Quit: Leaving.)
2019-07-11T15:53:37  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-ujwxtgwgpirxyxnb> has joined #fluid-work
2019-07-11T16:15:29  <the-t-in-rtf> updating my dev cloud now
2019-07-11T16:21:35  <javjarfer[m]> > sorry, [javjarfer](https://matrix.to/#/@javjarfer:matrix.org), just saw your message, my adium prompt didn't give an alert. deviceReporter has a get() invoker that returns a promise whose resolved value is solutions installed on that device
2019-07-11T16:21:36  <javjarfer[m]> np! thanks for the info cindyli
2019-07-11T16:22:24  <cindyli> anytime :)
2019-07-11T16:22:41  <jhernandez> cindyli: Tony's validation work has been "softly" reverted in master
2019-07-11T16:23:01  <cindyli> cool. thanks, jhernandez
2019-07-11T16:23:07  <jhernandez> so, dependency #1 is out of the way now
2019-07-11T16:24:05  <jhernandez> now we need clown's https://github.com/gpii-ops/gpii-infra/pull/383 to get in
2019-07-11T16:24:08  <jhernandez> right?
2019-07-11T16:24:18  <jhernandez> does it need any update on clown's side?
2019-07-11T16:24:30  * clown looks
2019-07-11T16:24:32  <cindyli> yes, dependency #2 and #3 and be resolved in parallel
2019-07-11T16:24:41  <jhernandez> cool
2019-07-11T16:25:11  <cindyli> jhernandez, Bosmon: i re-issued the pull request for GPII-3828 - https://github.com/GPII/universal/pull/803, so we don't waste this work
2019-07-11T16:25:36  <cindyli> i did test it with jhernandez's gpii-app branch and it worked well
2019-07-11T16:25:55  <cindyli> jhernandez: you may wanna do another round of testing when you have time, just in case
2019-07-11T16:25:56  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has quit IRC (Quit: danayo_)
2019-07-11T16:26:06  <jhernandez> of course
2019-07-11T16:26:09  <clown> jhernandez, cindyli:  look at the first note in pull 383, namely the repo and version of the universal image:  "repository: gpii/universal, tag: 20190627203047-1262ac8"
2019-07-11T16:26:43  <clown> those are taken from the-t-in-rtf's pull 438 (https://github.com/gpii-ops/gpii-infra/pull/438)
2019-07-11T16:26:57  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-ujwxtgwgpirxyxnb> has quit IRC (Quit: sepidehshahi)
2019-07-11T16:27:15  <clown> it might need to change.  we should touch base with stepan in slack.  I'll head on over there
2019-07-11T16:27:55  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has joined #fluid-work
2019-07-11T16:28:46  <jhernandez> sure, that's what I was going to suggest - thanks clown!
2019-07-11T16:28:58  <cindyli> ok, we'll need to get my universal pull request merged in first so that clown's tag can point to the most recent universal
2019-07-11T16:29:05  <clown> my pleasure.
2019-07-11T16:30:01  <clown> ok cindyli.  Then *technically* after that change, should we do a test run in my dev-cloud, Or yours? Or got striaght to stg?
2019-07-11T16:30:16  <cindyli> but jhernandez, will HA has problem with this change to defaultSettings.json5 in my pull request - https://github.com/GPII/universal/pull/801/files#diff-e1700c8cfcdc70a4191fbc90e16900d6R94?
2019-07-11T16:30:46  <cindyli> clown: it would be great to do around of test in our personal cloud before heading to stg
2019-07-11T16:30:49  *** colinclark <colinclark!~colinclar@205.211.168.103> has joined #fluid-work
2019-07-11T16:31:21  <Bosmon> Thanks cindyli - I remember when you first raised the pull, I was suspicious about a possible race at this line - https://github.com/GPII/universal/pull/803/files#diff-e28b72dafa8f53a9b7285f6ed50f7449R88
2019-07-11T16:31:26  <clown> yeah, thought so cindyli.
2019-07-11T16:31:29  *** kris_HA <kris_HA!~kris@95.111.73.80> has joined #fluid-work
2019-07-11T16:31:33  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-crdgoqmzbsbibahz> has joined #fluid-work
2019-07-11T16:32:20  <Bosmon> But I think that now we have adjusted for the original GPII-4005 race, I think that this race will be gone from your code too
2019-07-11T16:32:49  <cindyli> think so too. YAY!
2019-07-11T16:33:01  <Bosmon> I see that the ontologyHandler synchronously loads its data on startup which is initialised into a member
2019-07-11T16:33:10  <jhernandez> cindyli: which PR from you need to go in? isn't it there already?
2019-07-11T16:33:24  <Bosmon> So as long as there isn't a circle in progress, you should succeed in referencing the flat ontology from there
2019-07-11T16:33:47  <jhernandez> oh, nevermind, this one https://github.com/GPII/universal/pull/801
2019-07-11T16:34:15  <cindyli> jhernandez: it is ready here - https://github.com/GPII/universal/pull/801, but i remember kris mentioned he may have trouble to adjust to the new form of this line in defaultSettings.json in time - https://github.com/GPII/universal/pull/801/files#diff-e1700c8cfcdc70a4191fbc90e16900d6R94
2019-07-11T16:35:32  <cindyli> do you need to double check with HA first to find out if they are ok to accommodate this change before the deadline
2019-07-11T16:37:13  <jhernandez> cindyli: the update of the defaultSettings.json file in that PR is not a big problem
2019-07-11T16:37:43  <cindyli> great to know
2019-07-11T16:38:00  <jhernandez> remember that we can provide a defaultSettings one if by the end of the deadline kris has problems with it
2019-07-11T16:38:09  <cindyli> cool
2019-07-11T16:38:12  <jhernandez> *a different defaultSettings file
2019-07-11T16:38:15  <jhernandez> yup
2019-07-11T16:38:26  <jhernandez> so, don't worry about shipping that one
2019-07-11T16:38:54  <cindyli> super!
2019-07-11T16:40:04  <cindyli> jhernandez: now we can merge this pull request if you and Bosmon approve it
2019-07-11T16:40:06  <clown> brb *grabbing my lunch*
2019-07-11T16:44:54  * clown is back
2019-07-11T16:46:13  <jhernandez> cindyli: yeah, the PR looks good to me
2019-07-11T16:46:21  <jhernandez> I just left a question though
2019-07-11T16:46:37  <jhernandez> https://github.com/GPII/universal/pull/801/files#r302643085
2019-07-11T16:46:56  <cindyli> ya, see it
2019-07-11T16:46:59  <cindyli> replying
2019-07-11T16:47:53  <jhernandez> I'm sure it's okay, but just wanted to double check
2019-07-11T16:50:58  <jhernandez> cindyli: and yeah, take your time to deploy it into your dev cluster
2019-07-11T16:50:59  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-crdgoqmzbsbibahz> has quit IRC (Quit: sepidehshahi)
2019-07-11T16:51:23  <jhernandez> oh, and remember to merge current master into your branch
2019-07-11T16:52:13  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-xuvtwzmtzbjbveio> has joined #fluid-work
2019-07-11T16:52:49  <cindyli> thanks for the reminder. yes done
2019-07-11T16:52:56  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): Stats distribution:
2019-07-11T16:52:56  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): "Name","# requests","50%","66%","75%","80%","90%","95%","98%","99%","100%"
2019-07-11T16:52:56  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): "GET /preferences/carla",184,39,46,61,71,84,100,110,150,170
2019-07-11T16:52:56  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): "GET /preferences/nvda",221,40,48,63,72,89,99,120,120,170
2019-07-11T16:52:56  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): "GET /preferences/omar",201,38,45,59,70,87,100,130,140,190
2019-07-11T16:52:57  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): "GET /preferences/vladimir",212,38,43,53,59,80,97,110,120,290
2019-07-11T16:52:57  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): "GET /preferences/wayne",194,37,47,55,67,85,100,110,130,150
2019-07-11T16:52:58  <the-t-in-rtf> null_resource.locust_swarm_session (local-exec): "Total",1012,39,47,57,68,87,100,120,130,290
2019-07-11T16:53:41  <the-t-in-rtf> It's comparable to currrent prod.
2019-07-11T16:53:55  <the-t-in-rtf> https://github.com/gpii-ops/gpii-infra/pull/438#issuecomment-508454577
2019-07-11T16:54:00  <clown> are those the launch codes to start WWIII, the-t-in-rtf?
2019-07-11T16:54:08  <the-t-in-rtf> "Current Prod Total",770,31,35,45,56,79,93,110,130,290
2019-07-11T16:54:14  <cindyli> is it good?
2019-07-11T16:54:33  <the-t-in-rtf> yes.
2019-07-11T16:54:40  <cindyli> great!!
2019-07-11T16:55:25  <the-t-in-rtf> i.e. patched master indeed has virtually identical performaance with what we have in production
2019-07-11T16:55:44  <the-t-in-rtf> Average response times are all around 50ms
2019-07-11T16:58:01  <cindyli> very nice
2019-07-11T16:58:19  <clown> congrats, the-t-in-rtf
2019-07-11T16:58:44  <the-t-in-rtf> I didn't do anything but test it
2019-07-11T16:58:47  <the-t-in-rtf> the real fix is coming
2019-07-11T16:59:02  <jhernandez> the-t-in-rtf: awesome, thanks for testing it :)
2019-07-11T16:59:15  <the-t-in-rtf> but with @Bosmon's work master is now deployable at least from a performance perspective
2019-07-11T16:59:29  <clown> yes, well, sometimes testing is long and time consuming
2019-07-11T16:59:59  <jhernandez> clown: tell me about that!
2019-07-11T17:00:02  <jhernandez> :)
2019-07-11T17:00:22  <clown> jhernandez:  okay.  there was this time a few weeks ago, when ….
2019-07-11T17:00:30  <clown> ;-)
2019-07-11T17:00:32  <jhernandez> xDDD
2019-07-11T17:00:51  <jhernandez> not complaining
2019-07-11T17:00:56  <jhernandez> :P
2019-07-11T17:09:54  *** avtar <avtar!~avtar@ip-45-3-26-3.user.start.ca> has joined #fluid-work
2019-07-11T17:39:46  *** kris_HA <kris_HA!~kris@95.111.73.80> has quit IRC (Ping timeout: 246 seconds)
2019-07-11T17:40:16  *** kris_HA <kris_HA!~kris@95.111.73.80> has joined #fluid-work
2019-07-11T17:47:48  <cindyli> clown: do you have time to have a call to help me with the gpii-infra version update for the universal docker image
2019-07-11T17:49:54  *** kendraf <kendraf!Adium@nat/ocadu/x-qcblnnisnfccdljh> has left #fluid-work
2019-07-11T17:51:15  <clown> cindyli:  let me get my headphones…
2019-07-11T17:55:40  <cindyli> clown: i think i figured it out. thanks
2019-07-11T17:55:45  <clown> oh, good!
2019-07-11T17:56:14  <clown> Because, I have to make a call at 2pm, and I was going ask if you could wait a bit
2019-07-11T17:58:16  <cindyli> :)
2019-07-11T18:01:08  <Justin_o> simonjb: not sure if you've seen this. I've been using the Monoid font for my coding environment for a while and have liked it. I just remember now that I'm setting up my new machine. I wonder if it would be useful for C2LC. The font is open source. https://larsenwork.com/monoid/
2019-07-11T18:01:18  <clown> cindyli:   I'll be back in a bit, after the call, and see how you are coming along.
2019-07-11T18:01:37  *** clown is now known as clown_afk
2019-07-11T18:02:47  <cindyli> thanks, clown
2019-07-11T18:07:23  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-xuvtwzmtzbjbveio> has quit IRC (Quit: sepidehshahi)
2019-07-11T18:15:12  <simonjb> thanks Justin_o: I've seen Monoid and for C2LC we will definitely be looking at font options
2019-07-11T18:15:42  <Justin_o> cool
2019-07-11T18:20:01  <clown_afk> okay cindyli I'm back if you need anything.
2019-07-11T18:21:37  <cindyli> thanks, clown_afk
2019-07-11T18:21:42  *** clown_afk is now known as clown
2019-07-11T18:25:26  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-fgjafrovruxrqbnh> has joined #fluid-work
2019-07-11T18:42:57  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-fgjafrovruxrqbnh> has quit IRC (Quit: sepidehshahi)
2019-07-11T18:45:39  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-ubifxdalifnvpaaj> has joined #fluid-work
2019-07-11T18:51:49  *** simonjb <simonjb!~simonjb@205.211.168.102> has quit IRC ()
2019-07-11T18:59:07  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has quit IRC (Quit: danayo_)
2019-07-11T19:09:49  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-ubifxdalifnvpaaj> has quit IRC (Quit: sepidehshahi)
2019-07-11T19:18:11  <colinclark> hey Justin_o
2019-07-11T19:19:38  <colinclark> I was just catching up on emails, and read the thread between you and Mike Gifford
2019-07-11T19:19:50  <colinclark> and the Web Experience Toolkit issue he filed
2019-07-11T19:20:01  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has joined #fluid-work
2019-07-11T19:20:10  <colinclark> pretty interesting stuff
2019-07-11T19:21:33  <Justin_o> colinclark: yes, hopefully things work out with that
2019-07-11T19:21:52  <colinclark> and your response about the value of a built-in experience as opposed to trying to make changes outside the site’s context
2019-07-11T19:22:15  <colinclark> I almost wonder if a response on that ticket is worthwhile? Also mentioning the Chrome plugin
2019-07-11T19:22:35  <colinclark> btw I looked into the new Chromium version of Edge and apparently Chrome extensions will run in it unmodified...
2019-07-11T19:22:53  <colinclark> to the point where there’s a flag you can check to enable it to install extensions directly from Google’s Store :)
2019-07-11T19:23:11  <Justin_o> oh wow.. well I guess it really is just chrome reskinned
2019-07-11T19:23:34  <colinclark> well, it’s the Chromium engine
2019-07-11T19:25:25  <Justin_o> reminds me that we should revisit our browser support at some point.. I'm just not sure if we have projects that will require us to support legacy browsers
2019-07-11T19:25:54  <Justin_o> it's sounded like MS has done a lot to support Narator in Edge, and chrome now too
2019-07-11T19:26:07  <colinclark> time to ditch IE 11 :) :)
2019-07-11T19:26:10  <colinclark> maybe?
2019-07-11T19:26:14  <Justin_o> I wonder if they'll keep shipping IE 11 with Win 10
2019-07-11T19:26:20  <Justin_o> with all those fixes
2019-07-11T19:26:21  <Justin_o> haha
2019-07-11T19:26:34  <Justin_o> yah, that's generally my thoughts.. lets ditch it, maybe.
2019-07-11T19:26:36  <Justin_o> lol
2019-07-11T19:26:52  <colinclark> i say we just do it
2019-07-11T19:26:56  <colinclark> throw caution to the wind
2019-07-11T19:27:08  <colinclark> embrace the promise of the future
2019-07-11T19:27:59  <Justin_o> I think it's causing some constraints or workarounds for both JS and CSS work we have been doing.. but I feel like we might have needed even older browser support for a project, maybe it was for one of the SJRK partners... I can't recall now.
2019-07-11T19:28:19  <Justin_o> but honestly I'm not even sure what we could do if that was the case
2019-07-11T19:32:38  <colinclark> yeah, it’s hard to know
2019-07-11T19:32:46  <colinclark> and of course there will be sacrifices if we change our support
2019-07-11T19:35:37  <colinclark> but it seems like we could simplify and shed some older stuff if we were to drop it
2019-07-11T19:36:55  <Justin_o> I think so, I'm pretty sure I added a bunch of work arounds for things needed for UIO before my leave. And it doesn't support text-to-speech
2019-07-11T19:37:20  <Justin_o> I think alanharnum and gmoss ran into issues with CSS around grid too
2019-07-11T19:37:31  <colinclark> yeah
2019-07-11T19:51:03  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-lgkoxskdsozecwhv> has joined #fluid-work
2019-07-11T20:09:57  <Bosmon> Hey there Justin_o, colinclark
2019-07-11T20:10:03  <Bosmon> Great to see this discussion going on :)
2019-07-11T20:10:18  <Bosmon> To be clear, I wasn't proposing to ditch IE11 "particularly soon", but, say, perhaps by the end of the year
2019-07-11T20:10:36  <Bosmon> And to answer Justin_o's question about what we could think about in terms of mitigation strategies for those left behind, the answer, sadly, would be nothing
2019-07-11T20:10:37  *** michelled <michelled!~Adium@135-23-84-147.cpe.pppoe.ca> has quit IRC (Quit: Leaving.)
2019-07-11T20:10:49  <colinclark> Bosmon, I’m curious...
2019-07-11T20:11:01  <Bosmon> The very feature we want, precisely because it is so fundamental, could never be emulated by any of the typical tricks such as transpilers or whatever
2019-07-11T20:11:11  <Bosmon> A JS VM with proxies is just fundamentally unlike one without
2019-07-11T20:11:18  <Bosmon> So once we make the choice, there is simply no going back
2019-07-11T20:12:05  <Bosmon> See this answer: https://stackoverflow.com/questions/35025204/javascript-proxy-support-in-babel
2019-07-11T20:18:08  <Bosmon> colinclark - what were you curious about?
2019-07-11T20:24:54  <Justin_o> Bosmon: these proxies make JS objects declarative?
2019-07-11T20:25:14  <Bosmon> Well, I'm not too sure what "declarative" means : P
2019-07-11T20:25:20  <Bosmon> Even less these days than we ever were : P
2019-07-11T20:25:58  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-lgkoxskdsozecwhv> has quit IRC (Quit: sepidehshahi)
2019-07-11T20:26:02  <Bosmon> They just mean that the disparity in power between authors writing in code and authors writing in configuration is levelled
2019-07-11T20:26:34  <Justin_o> so we'd rework the framework to hook directly into these proxies instead of all of the tooling we have now?
2019-07-11T20:26:53  <Bosmon> As the framework starts to shift more and more work into the earlier, data-driven phase of instantiation, people writing in code have to write funny things in order to ensure that things they want to reference have been evaluated
2019-07-11T20:27:24  <Bosmon> Justin_o - well, the framework would be reworked very little, actually - this isn't a feature aimed at the framework itself, which doesn't need it, but instead for people writing Infusion applications
2019-07-11T20:28:03  <Justin_o> okay, I guess I'd need to see an example to fully understand it, but I'm also guessing this is more for new features that we don't have yet
2019-07-11T20:28:41  <Bosmon> It would just be a "courtesy" layered on top of the component objects that the framework hands out, such that if a piece of code tried to examine a part of the component that hadn't been evaluated yet, it would then get visited
2019-07-11T20:29:08  <Justin_o> when you say "courtesy" do you mean that it isn't required?
2019-07-11T20:29:33  <Bosmon> Well, the problem is, as soon as anyone starts to issue a reference that uses it, it would then be required : P
2019-07-11T20:30:04  <Bosmon> And given there is no very feasible way of analysing a piece of code to see if it makes use of the facility, as soon as it has been used by anyone, it becomes required everywhere
2019-07-11T20:31:18  <Justin_o> Bosmon: okay got you, so the framework proper would still be usable in IE 11 but possibly nothing built with it that had assumed the "courtesy" layer.
2019-07-11T20:31:26  <Bosmon> Right
2019-07-11T20:31:28  *** kris_HA <kris_HA!~kris@95.111.73.80> has quit IRC (Ping timeout: 272 seconds)
2019-07-11T20:31:35  <Justin_o> okay gotcha
2019-07-11T20:31:41  <Bosmon> So when I say, "there's nothing we can do", I mean there's nothing WE can do
2019-07-11T20:32:00  <Bosmon> Someone who had written an Infusion-consuming application that used the facility that didn't run in IE11 could always rewrite it so it didn't use the facility
2019-07-11T20:32:36  <Bosmon> And then we could then always just make a conditional include on IE11 based on, perhaps, sticking the part of Infusion that advertises the proxies in a separate file
2019-07-11T20:32:47  <Bosmon> But in practice, it doesn't feel like a very realistic way of working
2019-07-11T20:33:36  <Justin_o> do you have an estimate on how much overhead we have in Infusion both in terms of code size and performance hit for supporting IE11?
2019-07-11T20:33:43  <Bosmon> In theory we could partition the framework and test cases such that a core of them kept running on IE11, but it would be a rapidly dwindling core
2019-07-11T20:34:10  <Bosmon> And feels a lot like the similarly unrealistic kind of thinking that made us keep supporting "Infusion components" that just used Fluid.js rather than also FluidIoC.js for quite a while
2019-07-11T20:34:31  <Justin_o> we have such components?
2019-07-11T20:34:39  <Bosmon> Justin_o - not for years :)
2019-07-11T20:34:40  <Justin_o> are this for keyboard accessibility?
2019-07-11T20:34:43  <Justin_o> okay
2019-07-11T20:34:55  <Bosmon> But I think we did support them, say, as late as 2013 or so
2019-07-11T20:35:04  *** michelled <michelled!~Adium@135-23-84-147.cpe.pppoe.ca> has joined #fluid-work
2019-07-11T20:35:06  <Bosmon> Long beyond the point it was sane to do so : P
2019-07-11T20:35:36  <Bosmon> For your earlier question, I don't think it is really a code size or performance hit kind of issue - it's just that certain things will just fail, without being rewritten in odd-looking ways
2019-07-11T20:36:27  <Justin_o> I sort of wondering what the value would be to just remove all the supports from the code base as opposed to letting them sit around.
2019-07-11T20:36:41  <Justin_o> in particular if we moved off support earlier
2019-07-11T20:36:50  <Bosmon> Well there aren't really many supports - it's just that we're planning to explicitly break support
2019-07-11T20:37:22  <Bosmon> If we're talking about really old browser support, there are actually quite a few dirty corners of many of the components that are only there to support long-gone browsers
2019-07-11T20:37:33  <Bosmon> But there's very little in the core framework as it is now that is that way just to support IE11
2019-07-11T20:37:37  <Justin_o> I'd also be happy to not have to make new supports when I add new features to UIO ;)
2019-07-11T20:38:05  <colinclark> sorry, i had a meeting
2019-07-11T20:38:08  <colinclark> just catching up
2019-07-11T20:38:42  <Justin_o> yes, IE 11 wasn't as bad as some of the previous generations, it's mostly about lack of current features
2019-07-11T20:43:28  <Bosmon> Justin_o - so here is something fairly dreadful I had to do in our new core DataSource implementation, in order to skirt around our lack of proxies - https://github.com/amb26/infusion/blob/FLUID-6148/src/framework/core/js/DataSource.js#L330
2019-07-11T20:43:44  <Bosmon> The problem here is that the DataSource may be called upon to do some work before it is fully constructed
2019-07-11T20:44:10  <Bosmon> And in order to do this, it needs to evaluate some of its options which may not have been visited yet - but what's worse, the options it needs to visit might not even be fully known in advance
2019-07-11T20:44:42  <Bosmon> So, without proxies, eventually in the "new world", application code will have to perform these kinds of distasteful hacks, and also become aware of what really should be fairly private framework internals such as fluid.getForComponent
2019-07-11T20:46:16  <Bosmon> In the "old world" (that is, the world in which we are now), one didn't really expect to do much with a component before its onCreate had fired
2019-07-11T20:48:01  <Justin_o> yes... I feel like it's kind of hard to wrap my head around wanting to access features of a component before it was created, but then again in practice I've run into issues where I've wanted just that. I suppose with the proxies this will all be pretty seamless
2019-07-11T20:49:02  <Bosmon> Yes, it will also make the framework a good deal less confusing and require less knowledge by people trying to use it
2019-07-11T20:49:19  <Bosmon> It's much easier to internalise the rule "Any path you write in configuration will also work in code"
2019-07-11T20:49:55  <Bosmon> Rather than the current mishmash that "Many paths you write in configuration will work in code, unless you are in some kind a hard to anticipate load-time race"
2019-07-11T20:52:33  <Justin_o> +1 to "Any path you write in configuration will also work in code"
2019-07-11T20:52:59  *** jhung <jhung!~jhung@CPE54a0505a5e09-CMa84e3f431590.cpe.net.cable.rogers.com> has quit IRC (Quit: Leaving)
2019-07-11T20:55:09  *** michelled <michelled!~Adium@135-23-84-147.cpe.pppoe.ca> has quit IRC (Quit: Leaving.)
2019-07-11T20:56:03  *** cherylhjli <cherylhjli!~Adium@205.211.168.101> has quit IRC (Quit: Leaving.)
2019-07-11T20:56:45  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-fzmulgrymrtanaio> has joined #fluid-work
2019-07-11T20:59:07  *** simonjb <simonjb!~simonjb@205.211.168.102> has joined #fluid-work
2019-07-11T21:03:35  *** clown <clown!clown@nat/ocadu/x-xymhtlbiagihzxnf> has quit IRC (Quit: Leaving.)
2019-07-11T21:16:00  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-fzmulgrymrtanaio> has quit IRC (Quit: sepidehshahi)
2019-07-11T21:23:01  *** crystalchan <crystalchan!Adium@nat/ocadu/x-cnztdqwskjzynfws> has quit IRC (Quit: Leaving.)
2019-07-11T21:54:22  *** alanharnum <alanharnum!uid363993@gateway/web/irccloud.com/x-rrvjhczscdwtacld> has quit IRC (Quit: Connection closed for inactivity)
2019-07-11T22:00:40  *** avtar <avtar!~avtar@ip-45-3-26-3.user.start.ca> has quit IRC (Quit: Leaving.)
2019-07-11T22:02:32  *** cindyli <cindyli!~Adium@198.52.178.28> has quit IRC (Quit: Leaving.)
2019-07-11T22:07:25  *** simonjb <simonjb!~simonjb@205.211.168.102> has quit IRC ()
2019-07-11T22:08:15  *** colinclark <colinclark!~colinclar@205.211.168.103> has quit IRC (Quit: colinclark)
2019-07-11T22:15:29  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has quit IRC (Quit: danayo_)
2019-07-11T22:22:45  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has joined #fluid-work
2019-07-11T22:22:50  *** danayo_ <danayo_!~danayo@S0106f81d0f5cda43.vc.shawcable.net> has quit IRC (Client Quit)
2019-07-11T22:55:36  *** colinclark <colinclark!~colinclar@66-46-53-106.dedicated.allstream.net> has joined #fluid-work
2019-07-11T22:55:37  *** colinclark <colinclark!~colinclar@66-46-53-106.dedicated.allstream.net> has quit IRC (Client Quit)
2019-07-11T23:13:46  *** stegru <stegru!~ste@cpc120296-warr7-2-0-cust79.1-1.cable.virginm.net> has quit IRC (Ping timeout: 245 seconds)
2019-07-11T23:26:44  *** stegru <stegru!~ste@cpc120296-warr7-2-0-cust79.1-1.cable.virginm.net> has joined #fluid-work
