2018-12-11T00:30:35  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T00:57:29  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Quit: colinclark)
2018-12-11T02:19:25  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T02:23:51  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Read error: Connection reset by peer)
2018-12-11T02:25:49  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T04:10:25  *** danayo <danayo!~danayo@S01069050ca694f23.vc.shawcable.net> has quit IRC (Quit: danayo)
2018-12-11T04:24:08  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Quit: colinclark)
2018-12-11T04:27:10  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T04:27:10  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Client Quit)
2018-12-11T06:47:35  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3fc2:1300:d489:593a:1716:daea> has quit IRC (Remote host closed the connection)
2018-12-11T08:06:14  *** Bosmon2 <Bosmon2!~a@82-71-9-15.dsl.in-addr.zen.co.uk> has joined #fluid-work
2018-12-11T08:10:35  *** Bosmon <Bosmon!~a@82-71-9-15.dsl.in-addr.zen.co.uk> has quit IRC (Ping timeout: 250 seconds)
2018-12-11T08:53:06  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:d409:4300:a00b:a475> has quit IRC (Read error: Connection reset by peer)
2018-12-11T08:53:33  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:5850:70ca:9900:4a0> has joined #fluid-work
2018-12-11T08:55:38  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:5850:70ca:9900:4a0> has quit IRC (Client Quit)
2018-12-11T09:34:54  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has joined #fluid-work
2018-12-11T10:29:03  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:5912:67d0:78c1:a770> has joined #fluid-work
2018-12-11T10:29:03  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:5912:67d0:78c1:a770> has quit IRC (Client Quit)
2018-12-11T10:29:52  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:5912:67d0:78c1:a770> has joined #fluid-work
2018-12-11T10:29:54  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:5912:67d0:78c1:a770> has quit IRC (Client Quit)
2018-12-11T10:30:30  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:5912:67d0:78c1:a770> has joined #fluid-work
2018-12-11T12:28:53  <the-t-in-rtf> Morning @sgithens
2018-12-11T12:29:09  <the-t-in-rtf> Boz and I are around and chatting, just ping us when you're ready.
2018-12-11T12:31:55  <sgithens> On my way
2018-12-11T12:33:43  <sgithens> I"m the only one
2018-12-11T12:33:51  <sgithens> in the room
2018-12-11T12:48:32  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3fc2:1300:489:cd70:5b81:1cf4> has joined #fluid-work
2018-12-11T13:18:30  <jhernandez> stegru: hola
2018-12-11T13:21:32  <jhernandez> FYI - yesterday I spent the night trying to isolate the "exit code 3221226356" failure
2018-12-11T13:22:31  <jhernandez> I got nothing
2018-12-11T13:22:36  *** jhung <jhung!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has joined #fluid-work
2018-12-11T13:23:09  <jhernandez> BUT, the UWP code looks guilty to me
2018-12-11T13:23:35  <jhernandez> stegru: javjarfer[m] ^
2018-12-11T13:24:38  <jhernandez> also, I don't know why/how, but I ended up experiencing https://issues.gpii.net/browse/GPII-3558
2018-12-11T13:25:27  <jhernandez> parts of the system are in english, some others are in spanish
2018-12-11T13:44:22  *** cindyli <cindyli!~Adium@198.52.177.167> has joined #fluid-work
2018-12-11T13:53:38  <javjarfer[m]> jhernandez: oh, stegru was still looking to the issue again today
2018-12-11T13:53:58  <javjarfer[m]> anything special on why you think UWP is the issue?
2018-12-11T13:57:24  <jhernandez> javjarfer[m]: no idea
2018-12-11T13:57:40  <jhernandez> at this moment I can't reproduce the issue 100% anymore
2018-12-11T13:58:12  <jhernandez> the vm was restarted and now I got into GPII-3558
2018-12-11T13:58:41  <jhernandez> I've just added some information
2018-12-11T13:58:42  <javjarfer[m]> jhernandez: have you experienced it even with stegru 's pull?
2018-12-11T13:58:48  <jhernandez> yup
2018-12-11T13:59:20  <javjarfer[m]> <freenode_jhe "the vm was restarted and now I g"> that's nice, at least we were able to reproduce it
2018-12-11T13:59:50  <jhernandez> re the exit code failure
2018-12-11T14:00:01  <javjarfer[m]> <freenode_jhe "yup"> okay, then we can conclude the failure should be elsewhere
2018-12-11T14:00:59  <jhernandez> yes
2018-12-11T14:01:34  <jhernandez> but, during my tests, it seems that this allocation https://github.com/GPII/windows/blob/master/gpii/node_modules/registrySettingsHandler/test/testRegistrySettingsHandler.js#L500 triggers the problem
2018-12-11T14:02:21  <javjarfer[m]> the allocation itself?
2018-12-11T14:02:28  <jhernandez> yeah
2018-12-11T14:03:10  <jhernandez> to me, the allocation itself shouldn't be a problem
2018-12-11T14:07:39  <javjarfer[m]> yes, that's very weird...
2018-12-11T14:09:22  <javjarfer[m]> oh jhernandez stegru have you see this? https://github.com/TooTallNate/ref/issues/104
2018-12-11T14:12:12  <jhernandez> javjarfer[m]: hmmm
2018-12-11T14:13:36  *** alanharnum <alanharnum!~alanharnu@2607:fea8:12a0:b9b:30b9:773c:9e2a:d778> has joined #fluid-work
2018-12-11T14:18:08  <jhernandez> dunno, it might be a memory management issue in our code
2018-12-11T14:19:44  <Bosmon2> stegru, dandimitrov - as far as you are concerned, is GPII-3544 ready to go?
2018-12-11T14:23:22  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.104> has joined #fluid-work
2018-12-11T14:23:42  <stegru> yes Bosmon2
2018-12-11T14:24:57  <stegru> jhernandez, I thought i fixed it last night - it was working locally (honest!), but not on CI, or locally this morning
2018-12-11T14:26:14  *** avtar <avtar!~avtar@ip-24-50-165-63.user.start.ca> has joined #fluid-work
2018-12-11T14:26:18  *** avtar <avtar!~avtar@ip-24-50-165-63.user.start.ca> has quit IRC (Client Quit)
2018-12-11T14:26:32  *** avtar <avtar!~avtar@ip-24-50-165-63.user.start.ca> has joined #fluid-work
2018-12-11T14:28:15  *** clown <clown!clown@nat/ocadu/x-fyvkiuhycrzlahwo> has joined #fluid-work
2018-12-11T14:30:20  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has joined #fluid-work
2018-12-11T14:31:15  <stegru> and now that I've finished my pie, i will fix it
2018-12-11T14:36:24  <jhernandez> stegru: I did pull again late at night, after seeing CI failing again
2018-12-11T14:36:39  * jhernandez shrugs
2018-12-11T14:37:39  <Bosmon2> the-t-in-rtf - I notice our version of pouchdb in trunk is pretty stale, 6.4.3 compared to current 7.0.0
2018-12-11T14:37:51  <Bosmon2> Do you know if updating it is likely to lead to any particularly beneficial or hazardous results?
2018-12-11T14:45:50  *** simonjb <simonjb!~simonjb@198.178.118.18> has joined #fluid-work
2018-12-11T14:46:58  <the-t-in-rtf> friend
2018-12-11T14:47:02  <the-t-in-rtf> you just released a version with 7.0.0
2018-12-11T14:47:08  <the-t-in-rtf> or we did
2018-12-11T14:47:21  <the-t-in-rtf> it works fine with express-pouchdb and with our browser tests
2018-12-11T14:48:01  <the-t-in-rtf> https://github.com/GPII/gpii-pouchdb/blob/master/package.json#L25
2018-12-11T14:48:07  <the-t-in-rtf> unless you meant some other trunk
2018-12-11T14:52:28  <Bosmon2> the-t-in-rtf - yes, I see this now, sorry
2018-12-11T14:52:43  <Bosmon2> Just to confirm, we still expect to see these errors with this release, right?
2018-12-11T14:52:59  <Bosmon2> ERROR Renaming directory: { Error: EPERM: operation not permitted, rename 'C:\Users\Bosmon\AppData\Local\Temp\8m3fbx81-302279' -> 'C:\Users\
2018-12-11T14:52:59  <Bosmon2> Bosmon\AppData\Local\Temp\8m3fbx81-302279-OLD-1544539895474'
2018-12-11T14:52:59  <Bosmon2>     at Object.fs.renameSync (fs.js:766:18)
2018-12-11T14:52:59  <Bosmon2>     at Array.<anonymous> (E:\source\gits\gpii\node_modules\universal\node_modules\gpii-pouchdb\src\js\pouch-express.js:212:24)
2018-12-11T14:53:02  <the-t-in-rtf> yup
2018-12-11T14:53:10  <Bosmon2> ok
2018-12-11T14:53:13  <the-t-in-rtf> not in CI, but in Windows and VMs
2018-12-11T14:53:20  <the-t-in-rtf> (windows VMs)
2018-12-11T14:54:05  <jhung> fluid-everyone - there isn't a design crit today.
2018-12-11T14:56:01  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T15:15:15  *** jhung <jhung!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has quit IRC (Quit: Leaving.)
2018-12-11T15:16:17  *** jhung <jhung!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has joined #fluid-work
2018-12-11T15:17:26  <javjarfer[m]> stegru: good bug hunting, ping me if you want to test anything
2018-12-11T15:17:39  <javjarfer[m]> I want to help to trace that
2018-12-11T15:19:07  <stegru> don't take CI's job
2018-12-11T15:25:00  <stegru> jhernandez, anything else blocking your new build?
2018-12-11T15:25:28  <jhernandez> stegru: I don't think so
2018-12-11T15:25:52  <jhernandez> I was going to pull dandimitrov's branch and give it a try
2018-12-11T15:27:24  <jhernandez> and see if there are any other last minute integration problems
2018-12-11T15:31:22  <stegru> dandimitrov, have you done anything for GPII-3572 (data colleciton switch)? I'm asking because I'm going to start on it
2018-12-11T15:31:27  <dandimitrov> jhernandez, stegru: Languages seem to be working fine with the latest changes in GPII-3544
2018-12-11T15:31:39  <stegru> good
2018-12-11T15:32:26  <dandimitrov> and no, we havenâ€™t done anything for 3572
2018-12-11T15:33:03  <stegru> ok thanks
2018-12-11T15:44:23  <Bosmon2> the-t-in-rtf - didn't you make a fix somewhere which resolved this [undefined ms] business?
2018-12-11T15:44:39  <Bosmon2> I've tried the most recent dev release of gpii-testem and it still seems to be there
2018-12-11T15:56:42  *** Bosmon2 is now known as Bosmon
2018-12-11T15:58:10  <javjarfer[m]> <freenode_ste "don't take CI's job"> CI doesn't know how to properly blame stegru
2018-12-11T16:00:55  *** danayo <danayo!~danayo@S01069050ca694f23.vc.shawcable.net> has joined #fluid-work
2018-12-11T16:16:37  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.104> has quit IRC (Quit: sepidehshahi)
2018-12-11T16:19:59  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.104> has joined #fluid-work
2018-12-11T16:51:39  *** danayo_ <danayo_!~danayo@S01069050ca694f23.vc.shawcable.net> has joined #fluid-work
2018-12-11T16:59:40  <the-t-in-rtf> @Bosmon, I absolutely did.
2018-12-11T17:00:01  <the-t-in-rtf> It's a fix against QUnit, let me dig up the pull
2018-12-11T17:00:34  <the-t-in-rtf> It was merged
2018-12-11T17:00:34  <the-t-in-rtf> https://github.com/fluid-project/infusion/pull/943
2018-12-11T17:01:18  <the-t-in-rtf> Any dev release after then, say the 18th of November or later should no longer have the undefined messages.
2018-12-11T17:01:42  <the-t-in-rtf> it was never a testem error, which is why you still see it regardless of that version
2018-12-11T17:02:10  <the-t-in-rtf> it was testem expecting a more modern name for the data that newer versions of QUnit suppl
2018-12-11T17:02:12  <the-t-in-rtf> y
2018-12-11T17:02:37  <the-t-in-rtf> testem is just the messenger
2018-12-11T17:03:15  <the-t-in-rtf> or rather a dumb messenger that looks on the spot on the box where the label should be and blurts out "undefined" if there's no label
2018-12-11T17:09:25  *** jhung is now known as jhung_away
2018-12-11T17:20:12  <Bosmon> the-t-in-rtf - oh cool, thanks
2018-12-11T17:20:22  <Bosmon> cindyli - would you be able to take a look at https://github.com/GPII/universal/pull/716 ?
2018-12-11T17:20:27  <Bosmon> The ops team have already confirmed it works for them
2018-12-11T17:22:12  <jhernandez> dandimitrov: question
2018-12-11T17:22:28  <cindyli> sure, Bosmon
2018-12-11T17:23:00  <javjarfer[m]> Hi there Bosmon ! I think I have addressed most of the issues with the previously reviewed pulls, I was taking a look to GPII-3130 right now
2018-12-11T17:23:19  <Bosmon> Awesome, thanks cindyli
2018-12-11T17:23:25  <jhernandez> I should get a survey when I first save settings, right?
2018-12-11T17:24:37  <dandimitrov> jhernandez: yep, in case youâ€™re using an empty profile
2018-12-11T17:24:50  <jhernandez> yup, I just did that
2018-12-11T17:25:07  <jhernandez> I'm starting with npm start
2018-12-11T17:26:41  <jhernandez> anything else I should check? dunno, anything in siteConfig or the like?
2018-12-11T17:31:18  <jhernandez> dandimitrov: no errors in the log
2018-12-11T17:32:46  <jhernandez> but, I'm seeing this in the logs: https://pastebin.com/086xJUjG
2018-12-11T17:33:23  <jhernandez> Bosmon: cindyli looks like there's a context change happening ???
2018-12-11T17:34:20  <jhernandez> here's the prefsSet https://preferences.prd.gpii.net/preferences/5fcc718e-bcbe-43dd-b32b-001b3fcd917a - nothing wrong with it
2018-12-11T17:34:40  <jhernandez> why on hell is the contextManager doing such thing?
2018-12-11T17:36:41  <Bosmon> jhernandez - I think the message is just wrongly worded
2018-12-11T17:36:46  <Bosmon> cindyli added it in quite recently
2018-12-11T17:37:13  <cindyli> ya, i added the logging line
2018-12-11T17:37:51  <cindyli> the line is probably added at the wrong spot because the next line in jhernandez's log says: Active contexts calculated to be: gpii-default
2018-12-11T17:39:20  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Quit: colinclark)
2018-12-11T17:40:13  *** jhung_away <jhung_away!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has quit IRC (Quit: Leaving.)
2018-12-11T17:41:59  <Bosmon> cindyli - also it misinterprets the forceContext argument
2018-12-11T17:47:47  <cindyli> right. i will send a pull request to fix it
2018-12-11T17:50:41  <stegru> Bosmon, making gpii-app start only the metrics was really easy
2018-12-11T18:09:17  *** avtar <avtar!~avtar@ip-24-50-165-63.user.start.ca> has quit IRC (Quit: Leaving.)
2018-12-11T18:11:49  <Bosmon> stegru - brilliant
2018-12-11T18:12:08  <Bosmon> stegru - so the slight wrinkle is that Gregg is now asking for two independent "switches"
2018-12-11T18:12:10  *** jhung <jhung!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has joined #fluid-work
2018-12-11T18:12:24  <Bosmon> That is, for either metrics, or the main app, to be started independently, or both, or neither
2018-12-11T18:30:56  *** danayo_ <danayo_!~danayo@S01069050ca694f23.vc.shawcable.net> has quit IRC (Quit: danayo_)
2018-12-11T18:30:57  *** danayo <danayo!~danayo@S01069050ca694f23.vc.shawcable.net> has quit IRC (Quit: danayo)
2018-12-11T18:54:17  *** danayo <danayo!~danayo@S01069050ca694f23.vc.shawcable.net> has joined #fluid-work
2018-12-11T18:54:21  *** danayo_ <danayo_!~danayo@S01069050ca694f23.vc.shawcable.net> has joined #fluid-work
2018-12-11T19:21:48  <bryan_> I'm getting errors when running tests - unable to locate bindings file (ffi-napi)  https://pastebin.com/W5aqLvkz (relevant error) https://pastebin.com/cuh8JcGx (whole output)
2018-12-11T19:22:20  <bryan_> This is running inside the latest windows VM
2018-12-11T19:29:17  <Bosmon> Hey there bryan_ - thanks for pasting this
2018-12-11T19:29:46  <Bosmon> This is all pretty bizarre since the paths that you have done this from are shown as follows: C:\Users\byoun\projects\gpii\gpii-app
2018-12-11T19:29:52  <Bosmon> This path will not exist within the VM
2018-12-11T19:30:16  <Bosmon> Did you customise the VM in some way by setting up some kind of account, checking out some code, etc?
2018-12-11T19:30:23  <Bosmon> Otherwise, this looks like it is a path in your host VM
2018-12-11T19:30:30  <Bosmon> Either way, this is problematic : P
2018-12-11T19:33:56  <bryan_> That is a new VM, no modifications...but I did run the test locally to see if it worked there...maybe I copied from the wrong window
2018-12-11T19:35:33  <Bosmon> ok
2018-12-11T19:35:52  <Bosmon> If you ran it locally, the binaries would certainly fail
2018-12-11T19:36:20  <Bosmon> See if you can get the paste from inside the VM
2018-12-11T19:36:30  <Bosmon> You will need to enable the bidirectional clipboard in the VM's options menu
2018-12-11T19:36:32  <bryan_> I'm creating a new pastebin...same errors in both
2018-12-11T19:39:00  <bryan_> https://pastebin.com/b3tRtRvl
2018-12-11T19:39:31  <Bosmon> Cheers bryan_
2018-12-11T19:39:38  <Bosmon> Can you try doing the run from v:\ instead?
2018-12-11T19:39:49  <Bosmon> This is a network drive that automatically gets mounted inside the VM
2018-12-11T19:40:47  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T19:45:08  <bryan_> Hmmm...the system cannot find the drive specified
2018-12-11T19:45:14  <Bosmon> Ah
2018-12-11T19:45:21  <Bosmon> Well that's a suspicious sign
2018-12-11T19:46:06  <bryan_> Yeah maybe a virtual cable is unplugged lol
2018-12-11T19:46:15  <Bosmon> OK, so how did you get this VM started?
2018-12-11T19:46:19  *** avtar <avtar!~avtar@192-0-151-7.cpe.teksavvy.com> has joined #fluid-work
2018-12-11T19:47:09  <bryan_> Vagrant destroy, vagrant box update, vagrant up
2018-12-11T19:47:49  <bryan_> Let me restart it
2018-12-11T19:47:51  <bryan_> The vm
2018-12-11T19:49:08  <stegru> metrics kill switch is doable, Bosmon
2018-12-11T19:49:27  <bryan_> I have a feeling restarting it will clear out any cobwebs
2018-12-11T19:49:36  <Bosmon> bryan_ - ok - the fact that there is no V: drive is extremely puzzling
2018-12-11T19:49:46  <Bosmon> You didn't get any errors during the vagrant up process?
2018-12-11T19:49:52  <Bosmon> It can take an extremely long time
2018-12-11T19:50:47  <bryan_> No errors...it started up very quickly...<2 min and now I have access to the v: drive
2018-12-11T19:51:14  <Bosmon> bryan_ - ok great
2018-12-11T19:51:18  <bryan_> The test is running now, fingers crossed
2018-12-11T19:51:54  <Bosmon> vagrant up will only be very slow the very first time as it provisions the machine
2018-12-11T19:52:45  <stegru> you might want to take a snapshot of it, bryan_, so you don't need to do all this again
2018-12-11T19:52:56  <Bosmon> snapshot?
2018-12-11T19:53:05  <stegru> yes
2018-12-11T19:53:17  <stegru> host+t
2018-12-11T19:55:50  <bryan_> Thanks stegru I'll do that
2018-12-11T20:02:48  <Bosmon> bryan_ - test should be done by now I guess - did it pass?
2018-12-11T20:04:06  <bryan_> Just failed with the same error...unable to locate bindings
2018-12-11T20:05:13  <bryan_> Maybe I have to blow away node_modules and reinstall
2018-12-11T20:05:19  <bryan_> Let me try that
2018-12-11T20:05:23  <Bosmon> Sure
2018-12-11T20:05:37  <Bosmon> Also capture the output from npm install in the pastebin
2018-12-11T20:20:22  <Bosmon> bryan_ - how did it go?
2018-12-11T20:25:53  <bryan_> npm install looks like maybe 60% done...no errors so far
2018-12-11T20:26:31  <bryan_> I should have done --verbose though
2018-12-11T20:26:48  <Bosmon> 60% done!
2018-12-11T20:26:51  <Bosmon> In 20 minutes?
2018-12-11T20:26:56  <Bosmon> And I thought my machine was slow ....
2018-12-11T20:35:46  <bryan_> Could be the internet...I'm at lunch so I'm tethered to my phone
2018-12-11T20:37:14  <bryan_> I see some errors EBUSY: resource busy or locked
2018-12-11T20:37:29  <bryan_> I'll wait til it finishes and post the whole output
2018-12-11T20:37:34  <Bosmon> ok
2018-12-11T20:37:43  <Bosmon> Which dir did you run npm install from?
2018-12-11T20:47:20  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.104> has quit IRC (Quit: sepidehshahi)
2018-12-11T20:47:41  <bryan_> From the v: drive
2018-12-11T20:49:21  <Bosmon> ok
2018-12-11T20:49:27  <Bosmon> EBUSY is worrisome
2018-12-11T20:52:29  <bryan_> It is stalling here https://pastebin.com/FiyyrH2S
2018-12-11T20:52:41  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.104> has joined #fluid-work
2018-12-11T20:52:56  <Bosmon> That's pretty interesting
2018-12-11T20:53:06  <Bosmon> My first thought is that the npm package cache is corrupt somehow
2018-12-11T20:53:39  <Bosmon> But perhaps you just got a lot of broken transfers to the npm registry because of your tethering
2018-12-11T20:54:38  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Quit: colinclark)
2018-12-11T20:57:38  <bryan_> I'll be back home soon and I'll try it again on a better connection
2018-12-11T20:57:58  <Bosmon> As well as doing rm -rf node_modules in V: you should also blow away your npm cache
2018-12-11T20:58:06  <Bosmon> I'm just trying to find out where it is in the VM
2018-12-11T20:59:14  <Bosmon> Looks like it's in C:\Users\vagrant\AppData\Roaming\npm-cache
2018-12-11T20:59:34  <Bosmon> Destroy that completely too, and then do an npm install from V: on a good connection and see what happens
2018-12-11T20:59:53  <Bosmon> What versions of vagrant and virtualBox do you have installed?
2018-12-11T21:12:47  <bryan_> Vagrant 2.2.2, VirtualBox 5.2.22
2018-12-11T21:13:04  <Bosmon> Seems reasonable
2018-12-11T21:13:15  <bryan_> I'll try clearing the npm cache at home
2018-12-11T21:13:50  <Bosmon> My best guess is that you've just ended up with some corrupt tarballs from some broken transfers
2018-12-11T21:13:56  <Bosmon> But the EBUSY stuff is a bit baffling
2018-12-11T21:14:03  <Bosmon> Your outer OS is Windows 10 now, right?
2018-12-11T21:20:03  <Bosmon> javjarfer[m] - just a last bit of consolidation for https://github.com/GPII/windows/pull/186 I think
2018-12-11T21:20:08  <bryan_> Yeah windows 10
2018-12-11T21:20:38  <bryan_> Maybe a full restart for good measure
2018-12-11T21:27:17  <stegru> EBUSY types of errors are sometimes related to anti-virus
2018-12-11T21:28:00  <stegru> (I think it's disabled in the vm)
2018-12-11T21:32:23  *** sepidehshahi <sepidehshahi!~sepidehsh@205.211.168.104> has quit IRC (Quit: sepidehshahi)
2018-12-11T21:33:53  *** cindyli <cindyli!~Adium@198.52.177.167> has quit IRC (Quit: Leaving.)
2018-12-11T21:37:06  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has quit IRC (Quit: dandimitrov)
2018-12-11T21:39:58  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T21:53:31  *** alanharnum <alanharnum!~alanharnu@2607:fea8:12a0:b9b:30b9:773c:9e2a:d778> has quit IRC ()
2018-12-11T21:56:26  <Bosmon> stegru - errno 3221225725 !
2018-12-11T21:56:28  <Bosmon> Exotic
2018-12-11T21:57:12  <stegru> where's that from?
2018-12-11T21:57:28  <Bosmon> From your recently failed gpii-app test #454
2018-12-11T21:57:53  <Bosmon> Stack overflow suggests stack overflow :)
2018-12-11T21:57:58  <stegru> ha
2018-12-11T21:58:26  <stegru> that statement itself could cause a stack overflow
2018-12-11T22:01:44  *** clown <clown!clown@nat/ocadu/x-fyvkiuhycrzlahwo> has quit IRC (Quit: Leaving.)
2018-12-11T22:02:28  <stegru> jhernandez, I'm not debugging this crap until tomorrow... I know I'll take hours now finding something that's instantly visible in the morning
2018-12-11T22:02:47  *** simonjb <simonjb!~simonjb@198.178.118.18> has quit IRC ()
2018-12-11T22:08:58  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Quit: colinclark)
2018-12-11T22:13:28  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has joined #fluid-work
2018-12-11T22:15:25  *** colinclark <colinclark!~colinclar@c-76-101-154-19.hsd1.fl.comcast.net> has quit IRC (Client Quit)
2018-12-11T22:27:29  *** danayo_ <danayo_!~danayo@S01069050ca694f23.vc.shawcable.net> has quit IRC (Quit: danayo_)
2018-12-11T22:31:56  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has quit IRC (Quit: Leaving.)
2018-12-11T22:32:05  *** avtar <avtar!~avtar@192-0-151-7.cpe.teksavvy.com> has quit IRC (Quit: Leaving.)
2018-12-11T22:32:38  *** danayo <danayo!~danayo@S01069050ca694f23.vc.shawcable.net> has quit IRC (Quit: danayo)
2018-12-11T22:36:17  *** jhung <jhung!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has quit IRC (Quit: Leaving.)
2018-12-11T22:55:53  <sgithens>  jhernandez I'm assigning some of your capture jira's to myself since I'm working on the capture tool now
2018-12-11T23:05:04  <jhernandez> stegru: sure, heroes also need a rest from time to time - tty tomorrow ;)
2018-12-11T23:07:09  <jhernandez> sgithens: serve yourself
2018-12-11T23:08:01  <jhernandez> I received that legacy from Kasper, who was the default assignee
2018-12-11T23:09:43  <sgithens> jhernandez:  ahhhh, the legacy of KasperNET
2018-12-11T23:09:58  <jhernandez> yeah, the JIRA legacy
2018-12-11T23:10:11  <jhernandez> xD
2018-12-11T23:10:14  <sgithens> :)
2018-12-11T23:10:33  <sgithens> I still have that skinny thin dell laptop that we demo'd on in Linz
2018-12-11T23:11:09  <sgithens> for some reason it was mailed to me a few years ago... I think maybe for google I/O
2018-12-11T23:11:30  <jhernandez> you should put it on a frame and hang it on your wall :P
2018-12-11T23:12:45  <sgithens> I need some of the wooden hands and nfc rings to go with it
2018-12-11T23:37:27  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has joined #fluid-work
2018-12-11T23:53:06  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has quit IRC (Quit: Leaving.)
