2018-10-26T00:51:47  *** Justin_o <Justin_o!uid14648@gateway/web/irccloud.com/x-wguuiudgxqwfygtg> has quit IRC (Quit: Connection closed for inactivity)
2018-10-26T05:48:28  *** fluid-bot` <fluid-bot`!~limnoria@205.211.169.46> has joined #fluid-work
2018-10-26T05:49:18  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has quit IRC (Remote host closed the connection)
2018-10-26T05:50:01  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has joined #fluid-work
2018-10-26T05:50:30  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has quit IRC (Read error: Connection reset by peer)
2018-10-26T05:51:16  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has joined #fluid-work
2018-10-26T05:51:42  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has quit IRC (Read error: Connection reset by peer)
2018-10-26T05:52:26  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has joined #fluid-work
2018-10-26T05:53:30  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has quit IRC (Read error: Connection reset by peer)
2018-10-26T05:54:16  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has joined #fluid-work
2018-10-26T05:54:42  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has quit IRC (Read error: Connection reset by peer)
2018-10-26T05:55:26  *** [o__o] <[o__o]!~o__o]@ec2-54-85-45-223.compute-1.amazonaws.com> has joined #fluid-work
2018-10-26T05:55:51  *** fluid-bot <fluid-bot!~limnoria@205.211.169.46> has quit IRC (*.net *.split)
2018-10-26T06:15:37  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has joined #fluid-work
2018-10-26T06:22:40  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has quit IRC (Quit: dandimitrov)
2018-10-26T06:26:44  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has joined #fluid-work
2018-10-26T06:40:28  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has quit IRC (Quit: dandimitrov)
2018-10-26T06:41:58  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has joined #fluid-work
2018-10-26T06:51:53  *** dandimitrov <dandimitrov!~danailbd@87.121.114.143> has quit IRC (Quit: dandimitrov)
2018-10-26T07:40:50  *** dandimitrov <dandimitrov!~danailbd@37.157.190.158> has joined #fluid-work
2018-10-26T07:42:29  *** dandimitrov <dandimitrov!~danailbd@37.157.190.158> has quit IRC (Client Quit)
2018-10-26T07:49:02  *** dandimitrov <dandimitrov!~danailbd@37.157.190.158> has joined #fluid-work
2018-10-26T07:58:47  *** georgitodorov <georgitodorov!~georgitod@37.157.190.158> has joined #fluid-work
2018-10-26T09:02:47  *** stegru <stegru!~ste@cpc120296-warr7-2-0-cust79.1-1.cable.virginm.net> has quit IRC (Ping timeout: 240 seconds)
2018-10-26T09:14:30  *** stegru <stegru!~ste@cpc120296-warr7-2-0-cust79.1-1.cable.virginm.net> has joined #fluid-work
2018-10-26T10:10:17  *** dandimitrov <dandimitrov!~danailbd@37.157.190.158> has quit IRC (Quit: dandimitrov)
2018-10-26T11:08:32  *** Justin_o <Justin_o!uid14648@gateway/web/irccloud.com/x-mkapzvrgqeiuejco> has joined #fluid-work
2018-10-26T11:14:22  *** dandimitrov <dandimitrov!~danailbd@37.157.190.158> has joined #fluid-work
2018-10-26T11:37:41  *** dandimitrov <dandimitrov!~danailbd@37.157.190.158> has quit IRC (Quit: dandimitrov)
2018-10-26T12:36:41  *** cindyli <cindyli!~Adium@198.52.177.167> has joined #fluid-work
2018-10-26T12:53:44  *** the-t-in-rtf <the-t-in-rtf!~Adium@2a02:a210:2142:3480:7112:4a16:41de:4476> has quit IRC (Quit: Leaving.)
2018-10-26T12:56:52  *** colinclark <colinclark!~colinclar@135-23-98-140.cpe.pppoe.ca> has joined #fluid-work
2018-10-26T13:09:23  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has joined #fluid-work
2018-10-26T13:09:27  *** jhung <jhung!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has joined #fluid-work
2018-10-26T13:22:19  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-fhiafcedmhtxcbbx> has joined #fluid-work
2018-10-26T13:33:08  *** clown <clown!clown@nat/ocadu/x-cwzyyqotitkdirff> has joined #fluid-work
2018-10-26T13:41:14  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-zgittpgiwhwgjiso> has joined #fluid-work
2018-10-26T13:46:22  *** avtar <avtar!~avtar@ip-24-50-181-153.user.start.ca> has joined #fluid-work
2018-10-26T14:12:47  *** simonjb <simonjb!~simonjb@198.178.118.18> has joined #fluid-work
2018-10-26T15:04:17  *** georgitodorov <georgitodorov!~georgitod@37.157.190.158> has quit IRC (Quit: Leaving)
2018-10-26T15:25:51  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-zgittpgiwhwgjiso> has quit IRC (Remote host closed the connection)
2018-10-26T15:28:13  <jhernandez> stegru: hey
2018-10-26T15:28:16  <jhernandez> u around?
2018-10-26T15:29:55  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-ylwbvvaabkkpsvym> has joined #fluid-work
2018-10-26T15:59:23  <jhernandez> nevermind - had a question about metrics on kibana
2018-10-26T16:06:36  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-ylwbvvaabkkpsvym> has quit IRC (Remote host closed the connection)
2018-10-26T16:14:15  *** colinclark <colinclark!~colinclar@135-23-98-140.cpe.pppoe.ca> has quit IRC (Quit: colinclark)
2018-10-26T16:16:39  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has quit IRC (Quit: Leaving.)
2018-10-26T16:24:46  *** jhung is now known as jhung_away
2018-10-26T16:25:07  *** colinclark <colinclark!~colinclar@135-23-98-140.cpe.pppoe.ca> has joined #fluid-work
2018-10-26T16:36:58  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-fpwwkvwqesnmlyrb> has joined #fluid-work
2018-10-26T16:45:07  *** colinclark <colinclark!~colinclar@135-23-98-140.cpe.pppoe.ca> has quit IRC (Quit: colinclark)
2018-10-26T16:51:18  *** jhung_away is now known as jhung
2018-10-26T16:55:32  *** cindyli <cindyli!~Adium@198.52.177.167> has quit IRC (Quit: Leaving.)
2018-10-26T16:57:59  *** cindyli <cindyli!~Adium@198.52.177.167> has joined #fluid-work
2018-10-26T17:28:15  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3ea7:d200:78c2:67f9:e3a9:354c> has quit IRC (Quit: jhernandez)
2018-10-26T17:37:23  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-fpwwkvwqesnmlyrb> has quit IRC (Remote host closed the connection)
2018-10-26T17:43:28  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-ybalfqnxcwkoqznu> has joined #fluid-work
2018-10-26T17:59:12  *** colinclark <colinclark!~colinclar@205.211.168.103> has joined #fluid-work
2018-10-26T18:17:35  <sgithens> Bosmon: Do task(s) perform any sort of IoC resolution? https://github.com/fluid-project/infusion-docs/blob/master/src/documents/IoCTestingFramework.md#supported-fixture-records
2018-10-26T18:18:10  <sgithens> for the other types of things we always have the 2 versions func / funcName, and it would be nice similarly for tasks to be able to resolve the function returning the promise from some component
2018-10-26T18:22:59  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has joined #fluid-work
2018-10-26T18:44:21  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-ybalfqnxcwkoqznu> has quit IRC (Remote host closed the connection)
2018-10-26T18:55:02  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-wgcjrvhxydrridnn> has joined #fluid-work
2018-10-26T18:58:14  *** mrtyler <mrtyler!~mrtyler@69.163.91.192> has joined #fluid-work
2018-10-26T18:58:44  <mrtyler> clown: cindyli summoned me :)
2018-10-26T18:59:02  <mrtyler> i hear you are having trouble with dataloader
2018-10-26T18:59:03  <clown> mrtyler:  I was about to summon you myself.
2018-10-26T18:59:11  <clown> although "summon" may be too strong.
2018-10-26T18:59:30  <mrtyler> yes i was being facetious :)
2018-10-26T18:59:31  <clown> I'm having trouble starting my aws dev environment
2018-10-26T18:59:43  <clown> never mind the dataloader.
2018-10-26T19:00:26  <clown> I've gone through the whole sequence here:  https://github.com/gpii-ops/gpii-infra/blob/master/aws/README.md#my-cluster-is-messed-up-and-i-just-want-to-get-rid-of-it-so-i-can-start-over
2018-10-26T19:00:45  <clown> including the AWS console and deleting various "dev-clown" resources.
2018-10-26T19:01:03  <clown> But now "rake" does virtually nothing.
2018-10-26T19:01:36  <clown> the laste time I invoked it was as:  "rake —trace"
2018-10-26T19:01:59  <clown> I got:
2018-10-26T19:01:59  <clown> rake aborted!
2018-10-26T19:01:59  <clown> No Rakefile found (looking for: rakefile, Rakefile, rakefile.rb, Rakefile.rb)
2018-10-26T19:02:08  <clown> whence the rake file?
2018-10-26T19:02:55  <clown> anyhow, I've since gone to the top of that readme and verified that I have all the tools with the correct versions.
2018-10-26T19:03:26  <mrtyler> no Rakefile found sounds like you're in the wrong directory? what is your working directory?
2018-10-26T19:03:27  <clown> and have worked my way down to: https://github.com/gpii-ops/gpii-infra/blob/master/aws/README.md#provision-an-environment
2018-10-26T19:03:40  <clown> *smacks head really hard*
2018-10-26T19:05:35  <clown> yeah, I was in another "dev" environment …
2018-10-26T19:05:49  <clown> now that I'm in the correct one, the first error is "Error: Error loading state: state data in S3 does not have the expected content"
2018-10-26T19:06:26  <clown> and, it repeats for three time, then exits
2018-10-26T19:06:34  <clown> "it" being that error
2018-10-26T19:06:46  <mrtyler> i would start with 'rake clobber'
2018-10-26T19:07:46  <clown> rake clobber did its thing with no errors.
2018-10-26T19:07:51  <clown> now rake?
2018-10-26T19:07:57  <mrtyler> yes
2018-10-26T19:08:48  <clown> Same three errors.
2018-10-26T19:09:13  <mrtyler> next i would look at #4 at https://github.com/gpii-ops/gpii-infra/blob/master/aws/README.md#my-cluster-is-messed-up-and-i-just-want-to-get-rid-of-it-so-i-can-start-over
2018-10-26T19:09:13  <clown> I love this one: "…  you may need to manually verify the remote state and update the Digest value stored in the DynamoDB table to the following value: "  No value is provided.
2018-10-26T19:09:48  <mrtyler> clearing out dynamodb is one of the bullets under #4, so maybe this is the right track
2018-10-26T19:10:16  <clown> okay, but I'll check S3 first, just to be sure.  Hang on.
2018-10-26T19:10:33  <mrtyler> yeah i would clean them both out
2018-10-26T19:11:58  <clown> S3 shows no "dev-clown" stuff in gpii-terraform-state, gpii-terraform-state/prereqs, nor gpii-kubernetes-state
2018-10-26T19:12:19  <clown> I cleaned them out earlier today.  They appear to still be clean.
2018-10-26T19:13:36  <clown> I'm a little brain dead — remind me what to do re dynamodb
2018-10-26T19:13:57  <clown> how do I find out the ID and Path?
2018-10-26T19:15:39  <mrtyler> #6 under https://github.com/gpii-ops/gpii-infra/blob/master/aws/README.md#error-acquiring-the-state-lock
2018-10-26T19:16:11  <simonjb> hi Justin_o, are you working at the moment?
2018-10-26T19:16:57  <simonjb> I had a question about the FLUID-6298 PR
2018-10-26T19:17:14  <clown> yes I'm at step 2. in that section that says, "Find the ID and Path from the error message (see above)".
2018-10-26T19:17:21  <clown> But I don't have such an error message.
2018-10-26T19:17:48  <clown> so, where else do I look?
2018-10-26T19:21:00  <clown> or, how do I generate the error message?
2018-10-26T19:22:02  <mrtyler> you can jump directly to #6
2018-10-26T19:22:57  <clown> okay, trying that
2018-10-26T19:27:00  <clown> mrtyler:  DynamoDB does not show any tables.  I get a splash page "Amazon DynamoDB"  inviting me to create a table, or go to "Getting started guide", etc.  There is one link "Monitoring tables"  — I'll see where that one goes.
2018-10-26T19:29:29  *** stepan[rtf] <stepan[rtf]!4d667136@gateway/web/cgi-irc/kiwiirc.com/ip.77.102.113.54> has joined #fluid-work
2018-10-26T19:30:04  <stepan[rtf]> hi @clown Cindy mentioned some issue with dataloader?
2018-10-26T19:30:32  <clown> hi stepan.  I'm having troubles with just staring my AWS dev environment.
2018-10-26T19:30:44  <clown> I'm enjoying some hand holding from mrtyler
2018-10-26T19:31:09  <clown> *… just starting
2018-10-26T19:31:15  <stepan[rtf]> yeah, just catching up :)..
2018-10-26T19:31:25  <clown> although I have been staring a lot too...
2018-10-26T19:31:31  <stepan[rtf]> the new dataloader didn't make it in the infra code yet, just fyi guys
2018-10-26T19:32:44  <clown> mytyler:  clicking the links on that page just leads me to various documentation pages.  Looking closely at the url, it ends with "#gettingStarted".  Even if I delete that anchor, it just puts me back there.
2018-10-26T19:32:51  <clown> https://console.aws.amazon.com/dynamodb/home?region=us-east-1#gettingStarted:
2018-10-26T19:33:55  <clown> stepan[rtf]:  do you mean make it into master?  I'm running things off of your pull requests.
2018-10-26T19:34:13  <clown> well, I would run things from there, if I could start the environment...
2018-10-26T19:34:26  <stepan[rtf]> ok, I'll leave you to it guys, as my dinner just arrived and my dear gf demans some attention :), but if there's anything with DL just ping me and I'll catch up on that later
2018-10-26T19:34:34  <stepan[rtf]> yes mean master
2018-10-26T19:34:55  <clown> have a nice dinnner and weekend.  Your dear bg as well, stepan[rtf]
2018-10-26T19:35:01  <clown> *gf
2018-10-26T19:35:47  <stepan[rtf]> my branch - stepanstipl/gpii-infra/GPII-3391-parametrise-force-update *should* work
2018-10-26T19:35:58  <stepan[rtf]> I have tested on GCP so I can say it works there just fine
2018-10-26T19:35:59  * clown who did 'gf' become 'bg'???
2018-10-26T19:36:28  * clown and how did "how" become "who"/
2018-10-26T19:36:44  <clown> yes, stepan[rtf] that's the pull I'm taking about.
2018-10-26T19:37:05  * clown maybe I should switch to GCP.
2018-10-26T19:37:12  <stepan[rtf]> :D  on AWS the behaviour is slightly different - it should work when you create a new cluster, but if you're just upgrading existing one, the new dataloader won't get applied there
2018-10-26T19:37:42  <stepan[rtf]> (that's consistent with how it behaves now, as the dataloader batch job is never recreated)
2018-10-26T19:38:02  <clown> stepan[rtf]: At this point I've oblitereated all my dev resources on AWS, and am starting from scratch.  Although it appears something is wrong with my dynamodb.
2018-10-26T19:38:36  <mrtyler> clown: sounds like you're not starting from the aws dashboard and ended up at some other dynamodb page. https://us-east-2.console.aws.amazon.com/dynamodb/home?region=us-east-2
2018-10-26T19:39:03  <stepan[rtf]> I would recommend switch to GCP :), I think the experience is better...
2018-10-26T19:39:28  <clown> right, mrtyler.  From AWS dashboard, I clicked the DynamoDB service link (or I thought that's what I was doing).
2018-10-26T19:39:58  <mrtyler> when i do that, i am taken to the link i pasted. do you end up somewhere else?
2018-10-26T19:40:41  <mrtyler> anyway, the instructions look like they should work if you start from that link
2018-10-26T19:40:51  <mrtyler> but i am hanging around to see if that is the case :)
2018-10-26T19:41:39  <clown> aha.  pasting your link got me somewhere else that looks like it might be where I want to be.  Checking...
2018-10-26T19:43:11  <clown> some success — I found the "Items".  There are a lot.  Looking for "dev-clown" ones.
2018-10-26T19:45:01  <clown> I found and deleted three.  Double checking that there are no more.
2018-10-26T19:46:00  <clown> found two more.  any way to filter on "dev-clown"?
2018-10-26T19:49:41  <mrtyler> looks like there is a filter mechanism at the top of the page
2018-10-26T19:49:49  <mrtyler> the Items page
2018-10-26T19:50:59  <mrtyler> although i can't make it work so far
2018-10-26T19:51:49  <mrtyler> wait, it works. doesn't look like there are any remaining dev-clown locks
2018-10-26T19:52:36  <clown> I gave up on filters, and went for the simpler find-on-page (cmd-f).  I can't find any more dev-clown locks.
2018-10-26T19:53:08  <clown> so, time for another "rake"?
2018-10-26T19:53:41  <mrtyler> yes
2018-10-26T19:53:50  * clown crosses fingers
2018-10-26T19:53:55  <mrtyler> i cleaned up some cruft while i was looking at dynamodb
2018-10-26T19:54:35  <clown> "Plugin reinitialization required. Please run "terraform init".
2018-10-26T19:54:51  * clown doing that
2018-10-26T19:55:23  <mrtyler> that is not likely to work
2018-10-26T19:55:56  <clown> "Terraform has detected that the configuration specified for the backend
2018-10-26T19:55:56  <clown> has changed. Terraform will now check for existing state in the backends."
2018-10-26T19:56:06  <clown> so, yeah, it didn't work.
2018-10-26T19:56:15  <mrtyler> how about #6 at https://github.com/gpii-ops/gpii-infra/blob/master/aws/README.md#my-cluster-is-messed-up-and-i-just-want-to-get-rid-of-it-so-i-can-start-over
2018-10-26T19:56:22  <mrtyler> you can skip the first bullet
2018-10-26T19:56:22  <clown> it's asking me for "The name of the S3 bucket"
2018-10-26T19:56:35  * clown revisits #6
2018-10-26T19:56:45  <mrtyler> and then #7 and #8
2018-10-26T19:57:00  <mrtyler> and then, if that all goes well, 'rake'
2018-10-26T19:57:27  <clown> no dev-clown roles in AWS console...
2018-10-26T20:01:36  <clown> I've got as far as #7 (rake _destroy), and got an error message, mrtyler:
2018-10-26T20:01:38  <clown> Error copying source module: error downloading 'file:///var/folders/0h/_ftz_ssn5d770s7btbpkngs40000gn/T/rake-tmp/dev-clown-modules': source path error: stat /var/folders/0h/_ftz_ssn5d770s7btbpkngs40000gn/T/rake-tmp/dev-clown-modules: no such file or directory
2018-10-26T20:01:54  <clown> is that a problem?
2018-10-26T20:03:50  <mrtyler> hmm
2018-10-26T20:04:14  <mrtyler> nah, please ignore it and move on
2018-10-26T20:04:40  <mrtyler> i took a pass over aws and didn't see any leftover resources. i think you cleaned everything well
2018-10-26T20:04:58  <clown> okay, finished #8.  Here goes…
2018-10-26T20:05:46  * clown just call me the annihilator
2018-10-26T20:06:00  <mrtyler>  /sanick clown the_annihilator
2018-10-26T20:06:27  <mrtyler> clownihilator?
2018-10-26T20:06:47  <clown> :-)
2018-10-26T20:07:40  <clown> no errors yet mrtyler.  It's got a lot further.
2018-10-26T20:07:51  <mrtyler> great
2018-10-26T20:08:57  <clown> rats, an error:  "Error: Error applying plan:"  Something to do with keys.  I'll copy the relevant stuff without the sensitive information.
2018-10-26T20:09:11  <clown> * aws_key_pair.kubernetes-dev-clown-gpii-net-fb831a7d0371dbe15a5506af0caa6d24: 1 error(s) occurred:
2018-10-26T20:09:36  *** cindyli <cindyli!~Adium@198.52.177.167> has quit IRC (Quit: Leaving.)
2018-10-26T20:09:57  <clown> * aws_key_pair.kubernetes-dev-clown-gpii-net-STUFF: Error import KeyPair: InvalidKeyPair.Duplicate: The keypair 'kubernetes.dev-clown.gpii.net-STUFF' already exists.
2018-10-26T20:10:10  <clown> status code: 400, request id: STUFF
2018-10-26T20:10:55  <mrtyler> aha, this step is missing from the documentation! you'll need to manually remove your ssh key, under the IAM dashboard
2018-10-26T20:10:56  * clown is guessing as to what is sensitive
2018-10-26T20:11:13  <mrtyler> the good news is i don't think anything in that output is sensitive :)
2018-10-26T20:11:22  *** cindyli <cindyli!~Adium@198.52.177.167> has joined #fluid-work
2018-10-26T20:11:24  <clown> cool
2018-10-26T20:13:32  <mrtyler> sorry that's not where it is
2018-10-26T20:13:34  <mrtyler> it's here: https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2#KeyPairs:sort=keyName
2018-10-26T20:13:57  <clown> oh, I thought I had just found it under "users/Joseph…"
2018-10-26T20:14:05  <clown> okay, trying that url.
2018-10-26T20:14:55  <clown> deleted
2018-10-26T20:15:17  <clown> "rake" the sand again?
2018-10-26T20:15:56  <clown> or should I clean up a bit first, e.g., rake destroy; rake clobber ?
2018-10-26T20:16:04  <mrtyler> i added a new bullet under #6 https://github.com/gpii-ops/gpii-infra/tree/master/aws#my-cluster-is-messed-up-and-i-just-want-to-get-rid-of-it-so-i-can-start-over
2018-10-26T20:16:23  <mrtyler> no need to destroy or clobber, just rake again
2018-10-26T20:16:32  <clown> rake'ing
2018-10-26T20:19:12  <clown> chugging along… no errors yet.
2018-10-26T20:19:33  *** jhung <jhung!~Adium@dhcp-108-168-100-210.cable.user.start.ca> has quit IRC (Quit: Leaving.)
2018-10-26T20:23:33  <clown> perhaps an error:  it's picking up an old host key from my 'know_hosts' for api.dev-clown.gpii.net and warning me about a possible DNS snoop.
2018-10-26T20:24:10  <clown> nonetheless, it is still creating stuff and continuing on.
2018-10-26T20:24:58  <clown> a familiar message:  GPII key carla does not exist
2018-10-26T20:25:09  <clown> oddly a good sign, that.
2018-10-26T20:25:29  <clown> it means the cluster is coming together.
2018-10-26T20:26:48  *** sepidehshahi <sepidehshahi!sepidehsha@nat/ocadu/x-fhiafcedmhtxcbbx> has quit IRC (Quit: sepidehshahi)
2018-10-26T20:26:52  <clown> and, it's up.  Thanks mrtyler!
2018-10-26T20:27:26  <mrtyler> np
2018-10-26T20:27:44  <clown> whoa.  it looks like the dataloader did its thing, even.  Looking more closely.
2018-10-26T20:28:20  <mrtyler> congrats :)
2018-10-26T20:32:55  *** cindyli <cindyli!~Adium@198.52.177.167> has quit IRC (Quit: Leaving.)
2018-10-26T20:33:47  <clown> aha.  the database ran and loaded the views and snapsets, but did not exit.   It's still in a run state.  Hmmm….
2018-10-26T20:34:15  <clown> nope, I'm wrong.  It terminated.  I was looking in the wrong place.
2018-10-26T20:59:07  *** avtar <avtar!~avtar@ip-24-50-181-153.user.start.ca> has quit IRC (Quit: Leaving.)
2018-10-26T21:01:38  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-wgcjrvhxydrridnn> has quit IRC (Remote host closed the connection)
2018-10-26T21:02:12  <colinclark> oh wow, mrtyler has returned!
2018-10-26T21:02:17  <colinclark> i've missed that dude here :)
2018-10-26T21:02:30  <clown> have a good weekend, mrtyler.  I'm off.
2018-10-26T21:02:57  *** clown <clown!clown@nat/ocadu/x-cwzyyqotitkdirff> has quit IRC (Quit: Leaving.)
2018-10-26T21:05:52  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-rdkwugfdssxhifpv> has joined #fluid-work
2018-10-26T21:06:49  *** stepan[rtf] <stepan[rtf]!4d667136@gateway/web/cgi-irc/kiwiirc.com/ip.77.102.113.54> has quit IRC (Remote host closed the connection)
2018-10-26T21:12:10  <mrtyler> gg clown
2018-10-26T21:12:14  <mrtyler> hi colin :)
2018-10-26T21:21:19  *** alanharnum <alanharnum!alanharnum@nat/ocadu/x-rdkwugfdssxhifpv> has quit IRC ()
2018-10-26T21:41:56  *** michelled <michelled!~Adium@192-0-151-7.cpe.teksavvy.com> has quit IRC (Quit: Leaving.)
2018-10-26T21:54:19  *** colinclark <colinclark!~colinclar@205.211.168.103> has quit IRC (Quit: colinclark)
2018-10-26T22:41:23  *** simonjb <simonjb!~simonjb@198.178.118.18> has quit IRC ()
2018-10-26T22:54:22  *** mrtyler <mrtyler!~mrtyler@69.163.91.192> has quit IRC (Quit: leaving)
2018-10-26T23:12:24  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3ea7:d200:78c2:67f9:e3a9:354c> has joined #fluid-work
2018-10-26T23:12:59  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3ea7:d200:78c2:67f9:e3a9:354c> has quit IRC (Client Quit)
2018-10-26T23:13:13  *** jhernandez <jhernandez!~jhernande@2a02:a03f:3ea7:d200:78c2:67f9:e3a9:354c> has joined #fluid-work
